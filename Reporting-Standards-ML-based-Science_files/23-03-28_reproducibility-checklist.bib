
@article{sculley_winners_2018,
	title = {Winner's {Curse}? {On} {Pace}, {Progress}, and {Empirical} {Rigor}},
	shorttitle = {Winner's {Curse}?},
	url = {https://openreview.net/forum?id=rJWF0Fywf},
	abstract = {The field of ML is distinguished both by rapid innovation and rapid dissemination of results. While the pace of progress has been extraordinary by any measure, in this paper we explore potential issues that we believe to be arising as a result. In particular, we observe that the rate of empirical advancement may not have been matched by consistent increase in the level of empirical rigor across the field as a whole. This short position paper highlights examples where progress has actually been slowed as a result, offers thoughts on incentive structures currently at play, and gives suggestions as seeds for discussions on productive change.},
	language = {en},
	urldate = {2022-10-27},
	author = {Sculley, D. and Snoek, Jasper and Wiltschko, Alex and Rahimi, Ali},
	month = jun,
	year = {2018},
}

@article{bouthillier_accounting_2020,
	title = {Accounting for {Variance} in {Machine} {Learning} {Benchmarks}},
	abstract = {Strong empirical evidence that one machine-learning algorithm A outperforms another one B ideally calls for multiple trials optimizing the learning pipeline over sources of variation such as data sampling, augmentation, parameter initialization, and hyperparameters choices. This is prohibitively expensive, and corners are cut to reach conclusions. We model the whole benchmarking process, revealing that variance due to data sampling, parameter initialization and hyperparameter choice impact markedly the results. We analyze the predominant comparison methods used today in the light of this variance. We show a counter-intuitive result that adding more sources of variation to an imperfect estimator approaches better the ideal estimator at a 51× reduction in compute cost. Building on these results, we study the error rate of detecting improvements, on ﬁve different deep-learning tasks/architectures. This study leads us to propose recommendations for performance comparisons.},
	language = {en},
	author = {Bouthillier, Xavier and Delaunay, Pierre and Bronzi, Mirko and Trofimov, Assya and Nichyporuk, Brennan and Szeto, Justin and Sepah, Naz and Raff, Edward and Madan, Kanika and Voleti, Vikram and Kahou, Samira Ebrahimi and Michalski, Vincent and Serdyuk, Dmitriy and Arbel, Tal and Pal, Chris and Varoquaux, Gaël and Vincent, Pascal},
	year = {2020},
	pages = {23},
}

@inproceedings{schmidt_descending_2021,
	title = {Descending through a {Crowded} {Valley} - {Benchmarking} {Deep} {Learning} {Optimizers}},
	url = {https://proceedings.mlr.press/v139/schmidt21a.html},
	abstract = {Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of fifteen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing more than 50,000 individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we cannot discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific optimizers and parameter choices that generally lead to competitive results in our experiments: Adam remains a strong contender, with newer methods failing to significantly and consistently outperform it. Our open-sourced results are available as challenging and well-tuned baselines for more meaningful evaluations of novel optimization methods without requiring any further computational efforts.},
	language = {en},
	urldate = {2022-03-23},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Schmidt, Robin M. and Schneider, Frank and Hennig, Philipp},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {9367--9376},
}

@article{gundersen_state_2020,
	title = {State of the {Art}: {Reproducibility} in {Artificial} {Intelligence}},
	abstract = {Background: Research results in artiﬁcial intelligence (AI) are criticized for not being reproducible. Objective: To quantify the state of reproducibility of empirical AI research using six reproducibility metrics measuring three different degrees of reproducibility. Hypotheses: 1) AI research is not documented well enough to reproduce the reported results. 2) Documentation practices have improved over time. Method: The literature is reviewed and a set of variables that should be documented to enable reproducibility are grouped into three factors: Experiment, Data and Method. The metrics describe how well the factors have been documented for a paper. A total of 400 research papers from the conference series IJCAI and AAAI have been surveyed using the metrics. Findings: None of the papers document all of the variables. The metrics show that between 20\% and 30\% of the variables for each factor are documented. One of the metrics show statistically signiﬁcant increase over time while the others show no change. Interpretation: The reproducibility scores decrease with increased documentation requirements. Improvement over time is found. Conclusion: Both hypotheses are supported.},
	language = {en},
	journal = {AAAI},
	author = {Gundersen, Odd Erik},
	year = {2020},
	pages = {8},
}

@inproceedings{bouthillier_unreproducible_2019,
	title = {Unreproducible {Research} is {Reproducible}},
	url = {http://proceedings.mlr.press/v97/bouthillier19a.html},
	language = {en},
	urldate = {2021-07-26},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Bouthillier, Xavier and Laurent, César and Vincent, Pascal},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {725--734},
}

@article{beam_challenges_2020,
	title = {Challenges to the {Reproducibility} of {Machine} {Learning} {Models} in {Health} {Care}},
	volume = {323},
	issn = {0098-7484},
	url = {https://jamanetwork.com/journals/jama/fullarticle/2758612},
	doi = {10.1001/jama.2019.20866},
	language = {en},
	number = {4},
	urldate = {2022-10-27},
	journal = {JAMA},
	author = {Beam, Andrew L. and Manrai, Arjun K. and Ghassemi, Marzyeh},
	month = jan,
	year = {2020},
	pages = {305},
}

@article{breiman_statistical_2001,
	title = {Statistical {Modeling}: {The} {Two} {Cultures} (with comments and a rejoinder by the author)},
	volume = {16},
	issn = {0883-4237, 2168-8745},
	shorttitle = {Statistical {Modeling}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.full},
	doi = {10.1214/ss/1009213726},
	abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
	number = {3},
	urldate = {2021-05-13},
	journal = {Statistical Science},
	author = {Breiman, Leo},
	month = aug,
	year = {2001},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {199--231},
	file = {Snapshot:C\:\\Users\\pthan\\Zotero\\storage\\GJ24FUDM\\1009213726.html:text/html},
}

@article{wiemken_machine_2020,
	title = {Machine {Learning} in {Epidemiology} and {Health} {Outcomes} {Research}},
	volume = {41},
	issn = {0163-7525, 1545-2093},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-publhealth-040119-094437},
	doi = {10.1146/annurev-publhealth-040119-094437},
	abstract = {Machine learning approaches to modeling of epidemiologic data are becoming increasingly more prevalent in the literature. These methods have the potential to improve our understanding of health and opportunities for intervention, far beyond our past capabilities. This article provides a walkthrough for creating supervised machine learning models with current examples from the literature. From identifying an appropriate sample and selecting features through training, testing, and assessing performance, the end-to-end approach to machine learning can be a daunting task. We take the reader through each step in the process and discuss novel concepts in the area of machine learning, including identifying treatment effects and explaining the output from machine learning models.},
	language = {en},
	number = {1},
	urldate = {2022-10-27},
	journal = {Annual Review of Public Health},
	author = {Wiemken, Timothy L. and Kelley, Robert R.},
	month = apr,
	year = {2020},
	pages = {21--36},
}

@article{varian_big_2014,
	title = {Big {Data}: {New} {Tricks} for {Econometrics}},
	volume = {28},
	issn = {0895-3309},
	shorttitle = {Big {Data}},
	url = {https://www.aeaweb.org/articles?id=10.1257/jep.28.2.3},
	doi = {10.1257/jep.28.2.3},
	abstract = {Computers are now involved in many economic transactions and can capture data associated with these transactions, which can then be manipulated and analyzed. Conventional statistical and econometric techniques such as regression often work well, but there are issues unique to big datasets that may require different tools. First, the sheer size of the data involved may require more powerful data manipulation tools. Second, we may have more potential predictors than appropriate for estimation, so we need to do some kind of variable selection. Third, large datasets may allow for more flexible relationships than simple linear models. Machine learning techniques such as decision trees, support vector machines, neural nets, deep learning, and so on may allow for more effective ways to model complex relationships. In this essay, I will describe a few of these tools for manipulating and analyzing big data. I believe that these methods have a lot to offer and should be more widely known and used by economists.},
	language = {en},
	number = {2},
	urldate = {2022-10-27},
	journal = {Journal of Economic Perspectives},
	author = {Varian, Hal R.},
	month = may,
	year = {2014},
	keywords = {Modeling with Large Data Sets},
	pages = {3--28},
}

@article{yarkoni_choosing_2017,
	title = {Choosing {Prediction} {Over} {Explanation} in {Psychology}: {Lessons} {From} {Machine} {Learning}},
	volume = {12},
	issn = {1745-6916},
	shorttitle = {Choosing {Prediction} {Over} {Explanation} in {Psychology}},
	url = {https://doi.org/10.1177/1745691617693393},
	doi = {10.1177/1745691617693393},
	abstract = {Psychology has historically been concerned, first and foremost, with explaining the causal mechanisms that give rise to behavior. Randomized, tightly controlled experiments are enshrined as the gold standard of psychological research, and there are endless investigations of the various mediating and moderating variables that govern various behaviors. We argue that psychology’s near-total focus on explaining the causes of behavior has led much of the field to be populated by research programs that provide intricate theories of psychological mechanism but that have little (or unknown) ability to predict future behaviors with any appreciable accuracy. We propose that principles and techniques from the field of machine learning can help psychology become a more predictive science. We review some of the fundamental concepts and tools of machine learning and point out examples where these concepts have been used to conduct interesting and important psychological research that focuses on predictive research questions. We suggest that an increased focus on prediction, rather than explanation, can ultimately lead us to greater understanding of behavior.},
	language = {en},
	number = {6},
	urldate = {2021-05-17},
	journal = {Perspectives on Psychological Science},
	author = {Yarkoni, Tal and Westfall, Jacob},
	month = nov,
	year = {2017},
	note = {Publisher: SAGE Publications Inc},
	keywords = {explanation, machine learning, prediction},
	pages = {1100--1122},
}

@article{tonidandel_big_2018,
	title = {Big {Data} {Methods}: {Leveraging} {Modern} {Data} {Analytic} {Techniques} to {Build} {Organizational} {Science}},
	volume = {21},
	issn = {1094-4281},
	shorttitle = {Big {Data} {Methods}},
	url = {https://doi.org/10.1177/1094428116677299},
	doi = {10.1177/1094428116677299},
	abstract = {Advances in data science, such as data mining, data visualization, and machine learning, are extremely well-suited to address numerous questions in the organizational sciences given the explosion of available data. Despite these opportunities, few scholars in our field have discussed the specific ways in which the lens of our science should be brought to bear on the topic of big data and big data's reciprocal impact on our science. The purpose of this paper is to provide an overview of the big data phenomenon and its potential for impacting organizational science in both positive and negative ways. We identifying the biggest opportunities afforded by big data along with the biggest obstacles, and we discuss specifically how we think our methods will be most impacted by the data analytics movement. We also provide a list of resources to help interested readers incorporate big data methods into their existing research. Our hope is that we stimulate interest in big data, motivate future research using big data sources, and encourage the application of associated data science techniques more broadly in the organizational sciences.},
	language = {en},
	number = {3},
	urldate = {2021-05-13},
	journal = {Organizational Research Methods},
	author = {Tonidandel, Scott and King, Eden B. and Cortina, Jose M.},
	month = jul,
	year = {2018},
	note = {Publisher: SAGE Publications Inc},
	keywords = {big data, data analytics, data science, multivariate analysis, philosophy of science, quantitative, quantitative research},
	pages = {525--547},
}

@article{schrider_supervised_2018,
	title = {Supervised {Machine} {Learning} for {Population} {Genetics}: {A} {New} {Paradigm}},
	volume = {34},
	issn = {0168-9525},
	shorttitle = {Supervised {Machine} {Learning} for {Population} {Genetics}},
	url = {https://www.sciencedirect.com/science/article/pii/S0168952517302251},
	doi = {10.1016/j.tig.2017.12.005},
	abstract = {As population genomic datasets grow in size, researchers are faced with the daunting task of making sense of a flood of information. To keep pace with this explosion of data, computational methodologies for population genetic inference are rapidly being developed to best utilize genomic sequence data. In this review we discuss a new paradigm that has emerged in computational population genomics: that of supervised machine learning (ML). We review the fundamentals of ML, discuss recent applications of supervised ML to population genetics that outperform competing methods, and describe promising future directions in this area. Ultimately, we argue that supervised ML is an important and underutilized tool that has considerable potential for the world of evolutionary genomics.},
	language = {en},
	number = {4},
	urldate = {2021-05-13},
	journal = {Trends in Genetics},
	author = {Schrider, Daniel R. and Kern, Andrew D.},
	month = apr,
	year = {2018},
	pages = {301--312},
	file = {ScienceDirect Snapshot:C\:\\Users\\pthan\\Zotero\\storage\\K4JCN8HH\\S0168952517302251.html:text/html},
}

@article{athey_machine_2019,
	title = {Machine {Learning} {Methods} {That} {Economists} {Should} {Know} {About}},
	volume = {11},
	url = {https://doi.org/10.1146/annurev-economics-080217-053433},
	doi = {10.1146/annurev-economics-080217-053433},
	abstract = {We discuss the relevance of the recent machine learning (ML) literature for economics and econometrics. First we discuss the differences in goals, methods, and settings between the ML literature and the traditional econometrics and statistics literatures. Then we discuss some specific methods from the ML literature that we view as important for empirical researchers in economics. These include supervised learning methods for regression and classification, unsupervised learning methods, and matrix completion methods. Finally, we highlight newly developed methods at the intersection of ML and econometrics that typically perform better than either off-the-shelf ML or more traditional econometric methods when applied to particular classes of problems, including causal inference for average treatment effects, optimal policy estimation, and estimation of the counterfactual effect of price changes in consumer choice models.},
	number = {1},
	urldate = {2021-05-13},
	journal = {Annual Review of Economics},
	author = {Athey, Susan and Imbens, Guido W.},
	year = {2019},
	note = {\_eprint: https://doi.org/10.1146/annurev-economics-080217-053433},
	pages = {685--725},
}

@article{lundberg_what_2021,
	title = {What {Is} {Your} {Estimand}? {Defining} the {Target} {Quantity} {Connects} {Statistical} {Evidence} to {Theory}},
	volume = {86},
	issn = {0003-1224},
	shorttitle = {What {Is} {Your} {Estimand}?},
	url = {https://doi.org/10.1177/00031224211004187},
	doi = {10.1177/00031224211004187},
	abstract = {We make only one point in this article. Every quantitative study must be able to answer the question: what is your estimand? The estimand is the target quantity—the purpose of the statistical analysis. Much attention is already placed on how to do estimation; a similar degree of care should be given to defining the thing we are estimating. We advocate that authors state the central quantity of each analysis—the theoretical estimand—in precise terms that exist outside of any statistical model. In our framework, researchers do three things: (1) set a theoretical estimand, clearly connecting this quantity to theory; (2) link to an empirical estimand, which is informative about the theoretical estimand under some identification assumptions; and (3) learn from data. Adding precise estimands to research practice expands the space of theoretical questions, clarifies how evidence can speak to those questions, and unlocks new tools for estimation. By grounding all three steps in a precise statement of the target quantity, our framework connects statistical evidence to theory.},
	language = {en},
	number = {3},
	urldate = {2021-11-08},
	journal = {American Sociological Review},
	author = {Lundberg, Ian and Johnson, Rebecca and Stewart, Brandon M.},
	month = jun,
	year = {2021},
	keywords = {causal inference, descriptive inference, estimands, research design, social statistics},
	pages = {532--565},
}

@article{grimmer_machine_2021,
	title = {Machine {Learning} for {Social} {Science}: {An} {Agnostic} {Approach}},
	volume = {24},
	shorttitle = {Machine {Learning} for {Social} {Science}},
	url = {https://doi.org/10.1146/annurev-polisci-053119-015921},
	doi = {10.1146/annurev-polisci-053119-015921},
	abstract = {Social scientists are now in an era of data abundance, and machine learning tools are increasingly used to extract meaning from data sets both massive and small. We explain how the inclusion of machine learning in the social sciences requires us to rethink not only applications of machine learning methods but also best practices in the social sciences. In contrast to the traditional tasks for machine learning in computer science and statistics, when machine learning is applied to social scientific data, it is used to discover new concepts, measure the prevalence of those concepts, assess causal effects, and make predictions. The abundance of data and resources facilitates the move away from a deductive social science to a more sequential, interactive, and ultimately inductive approach to inference. We explain how an agnostic approach to machine learning methods focused on the social science tasks facilitates progress across a wide range of questions.},
	number = {1},
	urldate = {2021-06-02},
	journal = {Annual Review of Political Science},
	author = {Grimmer, Justin and Roberts, Margaret E. and Stewart, Brandon M.},
	year = {2021},
	note = {\_eprint: https://doi.org/10.1146/annurev-polisci-053119-015921},
	pages = {395--419},
}

@article{malik_hierarchy_2020,
	title = {A {Hierarchy} of {Limitations} in {Machine} {Learning}},
	url = {https://arxiv.org/abs/2002.05193v2},
	abstract = {"All models are wrong, but some are useful", wrote George E. P. Box (1979). Machine learning has focused on the usefulness of probability models for prediction in social systems, but is only now coming to grips with the ways in which these models are wrong---and the consequences of those shortcomings. This paper attempts a comprehensive, structured overview of the specific conceptual, procedural, and statistical limitations of models in machine learning when applied to society. Machine learning modelers themselves can use the described hierarchy to identify possible failure points and think through how to address them, and consumers of machine learning models can know what to question when confronted with the decision about if, where, and how to apply machine learning. The limitations go from commitments inherent in quantification itself, through to showing how unmodeled dependencies can lead to cross-validation being overly optimistic as a way of assessing model performance.},
	language = {en},
	urldate = {2022-02-23},
	author = {Malik, Momin M.},
	month = feb,
	year = {2020},
	file = {Snapshot:C\:\\Users\\pthan\\Zotero\\storage\\AP9GMUW9\\2002.html:text/html},
}

@article{obermeyer_dissecting_2019,
	title = {Dissecting {Racial} {Bias} in an {Algorithm} {Used} {To} {Manage} the {Health} of {Populations}},
	volume = {366},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.aax2342},
	doi = {10.1126/science.aax2342},
	abstract = {Racial bias in health algorithms 
             
              The U.S. health care system uses commercial algorithms to guide health decisions. Obermeyer 
              et al. 
              find evidence of racial bias in one widely used algorithm, such that Black patients assigned the same level of risk by the algorithm are sicker than White patients (see the Perspective by Benjamin). The authors estimated that this racial bias reduces the number of Black patients identified for extra care by more than half. Bias occurs because the algorithm uses health costs as a proxy for health needs. Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care. 
             
             
              Science 
              , this issue p. 
              447 
              ; see also p. 
              421 
             
          ,  
            A health algorithm that uses health costs as a proxy for health needs leads to racial bias against Black patients. 
          ,  
            Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5\%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.},
	language = {en},
	number = {6464},
	urldate = {2023-03-16},
	journal = {Science},
	author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
	month = oct,
	year = {2019},
	pages = {447--453},
}

@article{nowok_synthpop_2016,
	title = {synthpop : {Bespoke} {Creation} of {Synthetic} {Data} in {R}},
	volume = {74},
	issn = {1548-7660},
	shorttitle = {\textbf{synthpop}},
	url = {http://www.jstatsoft.org/v74/i11/},
	doi = {10.18637/jss.v074.i11},
	language = {en},
	number = {11},
	urldate = {2023-03-16},
	journal = {Journal of Statistical Software},
	author = {Nowok, Beata and Raab, Gillian M. and Dibben, Chris},
	year = {2016},
}

@inproceedings{raji_fallacy_2022,
	address = {Seoul Republic of Korea},
	title = {The {Fallacy} of {AI} {Functionality}},
	isbn = {978-1-4503-9352-2},
	url = {https://dl.acm.org/doi/10.1145/3531146.3533158},
	doi = {10.1145/3531146.3533158},
	language = {en},
	urldate = {2023-03-16},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Raji, Inioluwa Deborah and Kumar, I. Elizabeth and Horowitz, Aaron and Selbst, Andrew},
	month = jun,
	year = {2022},
	pages = {959--972},
}

@article{liao_are_2021,
	title = {Are {We} {Learning} {Yet}? {A} {Meta} {Review} of {Evaluation} {Failures} {Across} {Machine} {Learning}},
	volume = {1},
	shorttitle = {Are {We} {Learning} {Yet}?},
	url = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/757b505cfd34c64c85ca5b5690ee5293-Abstract-round2.html},
	language = {en},
	urldate = {2023-03-16},
	journal = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
	author = {Liao, Thomas and Taori, Rohan and Raji, Deborah and Schmidt, Ludwig},
	month = dec,
	year = {2021},
}

@inproceedings{hullman_worst_2022,
	address = {Oxford United Kingdom},
	title = {The {Worst} of {Both} {Worlds}: {A} {Comparative} {Analysis} of {Errors} in {Learning} from {Data} in {Psychology} and {Machine} {Learning}},
	isbn = {978-1-4503-9247-1},
	shorttitle = {The {Worst} of {Both} {Worlds}},
	url = {https://dl.acm.org/doi/10.1145/3514094.3534196},
	doi = {10.1145/3514094.3534196},
	language = {en},
	urldate = {2023-03-16},
	booktitle = {Proceedings of the 2022 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Hullman, Jessica and Kapoor, Sayash and Nanayakkara, Priyanka and Gelman, Andrew and Narayanan, Arvind},
	month = jul,
	year = {2022},
	pages = {335--348},
}

@article{finlayson_clinician_2021,
	title = {The {Clinician} and {Dataset} {Shift} in {Artificial} {Intelligence}},
	volume = {385},
	issn = {0028-4793, 1533-4406},
	url = {http://www.nejm.org/doi/10.1056/NEJMc2104626},
	doi = {10.1056/NEJMc2104626},
	language = {en},
	number = {3},
	urldate = {2023-03-16},
	journal = {New England Journal of Medicine},
	author = {Finlayson, Samuel G. and Subbaswamy, Adarsh and Singh, Karandeep and Bowers, John and Kupke, Annabel and Zittrain, Jonathan and Kohane, Isaac S. and Saria, Suchi},
	month = jul,
	year = {2021},
	pages = {283--286},
}

@article{amrhein_scientists_2019,
	title = {Scientists {Rise} {Up} {Against} {Statistical} {Significance}},
	volume = {567},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/d41586-019-00857-9},
	doi = {10.1038/d41586-019-00857-9},
	language = {en},
	number = {7748},
	urldate = {2023-03-16},
	journal = {Nature},
	author = {Amrhein, Valentin and Greenland, Sander and McShane, Blake},
	month = mar,
	year = {2019},
	pages = {305--307},
}

@article{simmonds_how_2022,
	title = {How {Is} {Model}-{Related} {Uncertainty} {Quantified} and {Reported} in {Different} {Disciplines}?},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2206.12179},
	doi = {10.48550/ARXIV.2206.12179},
	abstract = {How do we know how much we know? Quantifying uncertainty associated with our modelling work is the only way we can answer how much we know about any phenomenon. With quantitative science now highly influential in the public sphere and the results from models translating into action, we must support our conclusions with sufficient rigour to produce useful, reproducible results. Incomplete consideration of model-based uncertainties can lead to false conclusions with real world impacts. Despite these potentially damaging consequences, uncertainty consideration is incomplete both within and across scientific fields. We take a unique interdisciplinary approach and conduct a systematic audit of model-related uncertainty quantification from seven scientific fields, spanning the biological, physical, and social sciences. Our results show no single field is achieving complete consideration of model uncertainties, but together we can fill the gaps. We propose opportunities to improve the quantification of uncertainty through use of a source framework for uncertainty consideration, model type specific guidelines, improved presentation, and shared best practice. We also identify shared outstanding challenges (uncertainty in input data, balancing trade-offs, error propagation, and defining how much uncertainty is required). Finally, we make nine concrete recommendations for current practice (following good practice guidelines and an uncertainty checklist, presenting uncertainty numerically, and propagating model-related uncertainty into conclusions), future research priorities (uncertainty in input data, quantifying uncertainty in complex models, and the importance of missing uncertainty in different contexts), and general research standards across the sciences (transparency about study limitations and dedicated uncertainty sections of manuscripts).},
	urldate = {2023-03-16},
	author = {Simmonds, Emily G. and Adjei, Kwaku Peprah and Andersen, Christoffer Wold and Aspheim, Janne Cathrin Hetle and Battistin, Claudia and Bulso, Nicola and Christensen, Hannah and Cretois, Benjamin and Cubero, Ryan and Davidovich, Ivan A. and Dickel, Lisa and Dunn, Benjamin and Dunn-Sigouin, Etienne and Dyrstad, Karin and Einum, Sigurd and Giglio, Donata and Gjerlow, Haakon and Godefroidt, Amelie and Gonzalez-Gil, Ricardo and Cogno, Soledad Gonzalo and Grosse, Fabian and Halloran, Paul and Jensen, Mari F. and Kennedy, John James and Langsaether, Peter Egge and Laverick, Jack H. and Lederberger, Debora and Li, Camille and Mandeville, Elizabeth and Mandeville, Caitlin and Moe, Espen and Schroder, Tobias Navarro and Nunan, David and Parada, Jorge Sicacha and Simpson, Melanie Rae and Skarstein, Emma Sofie and Spensberger, Clemens and Stevens, Richard and Subramanian, Aneesh and Svendsen, Lea and Theisen, Ole Magnus and Watret, Connor and OHara, Robert B.},
	year = {2022},
	keywords = {Applications (stat.AP), Atmospheric and Oceanic Physics (physics.ao-ph), FOS: Biological sciences, FOS: Computer and information sciences, FOS: Physical sciences, Quantitative Methods (q-bio.QM)},
}

@article{ye_prediction_2018,
	title = {Prediction of {Incident} {Hypertension} {Within} the {Next} {Year}: {Prospective} {Study} {Using} {Statewide} {Electronic} {Health} {Records} and {Machine} {Learning}},
	volume = {20},
	issn = {1438-8871},
	shorttitle = {Prediction of {Incident} {Hypertension} {Within} the {Next} {Year}},
	url = {http://www.jmir.org/2018/1/e22/},
	doi = {10.2196/jmir.9268},
	language = {en},
	number = {1},
	urldate = {2023-03-16},
	journal = {Journal of Medical Internet Research},
	author = {Ye, Chengyin and Fu, Tianyun and Hao, Shiying and Zhang, Yan and Wang, Oliver and Jin, Bo and Xia, Minjie and Liu, Modi and Zhou, Xin and Wu, Qian and Guo, Yanting and Zhu, Chunqing and Li, Yu-Ming and Culver, Devore S and Alfreds, Shaun T and Stearns, Frank and Sylvester, Karl G and Widen, Eric and McElhinney, Doff and Ling, Xuefeng},
	month = jan,
	year = {2018},
	pages = {e22},
}

@article{chiavegatto_filho_data_2021,
	title = {Data {Leakage} in {Health} {Outcomes} {Prediction} {With} {Machine} {Learning}. {Comment} on “{Prediction} of {Incident} {Hypertension} {Within} the {Next} {Year}: {Prospective} {Study} {Using} {Statewide} {Electronic} {Health} {Records} and {Machine} {Learning}”},
	volume = {23},
	issn = {1438-8871},
	shorttitle = {Data {Leakage} in {Health} {Outcomes} {Prediction} {With} {Machine} {Learning}. {Comment} on “{Prediction} of {Incident} {Hypertension} {Within} the {Next} {Year}},
	url = {https://www.jmir.org/2021/2/e10969},
	doi = {10.2196/10969},
	language = {en},
	number = {2},
	urldate = {2023-03-16},
	journal = {Journal of Medical Internet Research},
	author = {Chiavegatto Filho, Alexandre and Batista, André Filipe De Moraes and Dos Santos, Hellen Geremias},
	month = feb,
	year = {2021},
	pages = {e10969},
}

@article{roberts_cross-validation_2017,
	title = {Cross-{Validation} {Strategies} for {Data} {With} {Temporal}, {Spatial}, {Hierarchical}, or {Phylogenetic} {Structure}},
	volume = {40},
	issn = {09067590},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/ecog.02881},
	doi = {10.1111/ecog.02881},
	language = {en},
	number = {8},
	urldate = {2023-03-16},
	journal = {Ecography},
	author = {Roberts, David R. and Bahn, Volker and Ciuti, Simone and Boyce, Mark S. and Elith, Jane and Guillera-Arroita, Gurutzeta and Hauenstein, Severin and Lahoz-Monfort, José J. and Schröder, Boris and Thuiller, Wilfried and Warton, David I. and Wintle, Brendan A. and Hartig, Florian and Dormann, Carsten F.},
	month = aug,
	year = {2017},
	pages = {913--929},
}

@inproceedings{hammerla_lets_2015,
	address = {Osaka Japan},
	title = {Let’s ({Not}) {Stick} {Together}: {Pairwise} {Similarity} {Biases} {Cross}-{Validation} in {Activity} {Recognition}},
	isbn = {978-1-4503-3574-4},
	shorttitle = {Let's (not) stick together},
	url = {https://dl.acm.org/doi/10.1145/2750858.2807551},
	doi = {10.1145/2750858.2807551},
	language = {en},
	urldate = {2023-03-16},
	booktitle = {Proceedings of the 2015 {ACM} {International} {Joint} {Conference} on {Pervasive} and {Ubiquitous} {Computing}},
	publisher = {ACM},
	author = {Hammerla, Nils Y. and Plötz, Thomas},
	month = sep,
	year = {2015},
	pages = {1041--1051},
}

@article{bergmeir_use_2012,
	title = {On the {Use} of {Cross}-{Validation} for {Time} {Series} {Predictor} {Evaluation}},
	volume = {191},
	issn = {00200255},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0020025511006773},
	doi = {10.1016/j.ins.2011.12.028},
	language = {en},
	urldate = {2023-03-16},
	journal = {Information Sciences},
	author = {Bergmeir, Christoph and Benítez, José M.},
	month = may,
	year = {2012},
	pages = {192--213},
}

@misc{malik_hierarchy_2020-1,
	title = {A {Hierarchy} of {Limitations} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/2002.05193},
	abstract = {"All models are wrong, but some are useful", wrote George E. P. Box (1979). Machine learning has focused on the usefulness of probability models for prediction in social systems, but is only now coming to grips with the ways in which these models are wrong---and the consequences of those shortcomings. This paper attempts a comprehensive, structured overview of the specific conceptual, procedural, and statistical limitations of models in machine learning when applied to society. Machine learning modelers themselves can use the described hierarchy to identify possible failure points and think through how to address them, and consumers of machine learning models can know what to question when confronted with the decision about if, where, and how to apply machine learning. The limitations go from commitments inherent in quantification itself, through to showing how unmodeled dependencies can lead to cross-validation being overly optimistic as a way of assessing model performance.},
	urldate = {2023-03-16},
	publisher = {arXiv},
	author = {Malik, Momin M.},
	month = feb,
	year = {2020},
	note = {arXiv:2002.05193 [cs, econ, math, stat]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Economics - Econometrics, G.3, I.6.4, J.4, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@article{vandewiele_overly_2021,
	title = {Overly {Optimistic} {Prediction} {Results} on {Imbalanced} {Data}: {A} {Case} {Study} of {Flaws} and {Benefits} {When} {Applying} {Over}-{Sampling}},
	volume = {111},
	issn = {09333657},
	shorttitle = {Overly optimistic prediction results on imbalanced data},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0933365720312525},
	doi = {10.1016/j.artmed.2020.101987},
	language = {en},
	urldate = {2023-03-16},
	journal = {Artificial Intelligence in Medicine},
	author = {Vandewiele, Gilles and Dehaene, Isabelle and Kovács, György and Sterckx, Lucas and Janssens, Olivier and Ongenae, Femke and De Backere, Femke and De Turck, Filip and Roelens, Kristien and Decruyenaere, Johan and Van Hoecke, Sofie and Demeester, Thomas},
	month = jan,
	year = {2021},
	pages = {101987},
}

@misc{kapoor_model_nodate,
	title = {Model {Info} {Sheets} for {Addressing} {Leakage}},
	url = {https://reproducible.cs.princeton.edu/#model-info-sheets},
	journal = {Leakage and the Reproducibility Crisis in ML-based Science},
	author = {Kapoor, Sayash and Narayanan, Arvind},
}

@misc{lones_how_2023,
	title = {How {To} {Avoid} {Machine} {Learning} {Pitfalls}: {A} {Guide} for {Academic} {Researchers}},
	shorttitle = {How to avoid machine learning pitfalls},
	url = {http://arxiv.org/abs/2108.02497},
	abstract = {This document is a concise outline of some of the common mistakes that occur when using machine learning, and what can be done to avoid them. Whilst it should be accessible to anyone with a basic understanding of machine learning techniques, it was originally written for research students, and focuses on issues that are of particular concern within academic research, such as the need to do rigorous comparisons and reach valid conclusions. It covers five stages of the machine learning process: what to do before model building, how to reliably build models, how to robustly evaluate models, how to compare models fairly, and how to report results.},
	urldate = {2023-03-16},
	publisher = {arXiv},
	author = {Lones, Michael A.},
	month = feb,
	year = {2023},
	note = {arXiv:2108.02497 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{lin_neural_2019,
	title = {The {Neural} {Hype} and {Comparisons} {Against} {Weak} {Baselines}},
	volume = {52},
	issn = {0163-5840},
	url = {https://dl.acm.org/doi/10.1145/3308774.3308781},
	doi = {10.1145/3308774.3308781},
	abstract = {Recently, the machine learning community paused in a moment of self-reflection. In a widelydiscussed paper at ICLR 2018, Sculley et al. [13] wrote: "We observe that the rate of empirical advancement may not have been matched by consistent increase in the level of empirical rigor across the field as a whole." Their primary complaint is the development of a "research and publication culture that emphasizes wins" (emphasis in original), which typically means "demonstrating that a new method beats previous methods on a given task or benchmark". An apt description might be "leaderboard chasing"-and for many vision and NLP tasks, this isn't a metaphor. There are literally centralized leaderboards1 that track incremental progress, down to the fifth decimal point, some persisting over years, accumulating dozens of entries. 
            Sculley et al. remind us that "the goal of science is not wins, but knowledge". The structure of the scientific enterprise today (pressure to publish, pace of progress, etc.) means that "winning" and "doing good science" are often not fully aligned. To wit, they cite a number of papers showing that recent advances in neural networks could very well be attributed to mundane issues like better hyperparameter optimization. Many results can't be reproduced, and some observed improvements might just be noise.},
	language = {en},
	number = {2},
	urldate = {2023-03-16},
	journal = {ACM SIGIR Forum},
	author = {Lin, Jimmy},
	month = jan,
	year = {2019},
	pages = {40--51},
}

@misc{noauthor_32_nodate,
	title = {3.2. {Tuning} the {Hyper}-{Parameters} of an {Estimator}},
	url = {https://scikit-learn/stable/modules/grid_search.html},
	abstract = {Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include C,...},
	language = {en},
	urldate = {2023-03-16},
	journal = {scikit-learn},
}

@article{sculley_winners_2018-1,
	title = {Winner's {Curse}? {On} {Pace}, {Progress}, and {Empirical} {Rigor}},
	shorttitle = {Winner's {Curse}?},
	url = {https://openreview.net/forum?id=rJWF0Fywf},
	abstract = {The field of ML is distinguished both by rapid innovation and rapid dissemination of results. While the pace of progress has been extraordinary by any measure, in this paper we explore potential issues that we believe to be arising as a result. In particular, we observe that the rate of empirical advancement may not have been matched by consistent increase in the level of empirical rigor across the field as a whole. This short position paper highlights examples where progress has actually been slowed as a result, offers thoughts on incentive structures currently at play, and gives suggestions as seeds for discussions on productive change.},
	language = {en},
	urldate = {2023-03-16},
	author = {Sculley, D. and Snoek, Jasper and Wiltschko, Alex and Rahimi, Ali},
	month = jun,
	year = {2018},
}

@misc{raschka_model_2020,
	title = {Model {Evaluation}, {Model} {Selection}, and {Algorithm} {Selection} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1811.12808},
	abstract = {The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-one-out cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F-test 5x2 cross-validation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.},
	urldate = {2023-03-16},
	publisher = {arXiv},
	author = {Raschka, Sebastian},
	month = nov,
	year = {2020},
	note = {arXiv:1811.12808 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{cawley_over-fitting_2010,
	title = {On {Over}-fitting in {Model} {Selection} and {Subsequent} {Selection} {Bias} in {Performance} {Evaluation}},
	volume = {11},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v11/cawley10a.html},
	abstract = {Model selection strategies for machine learning algorithms typically involve
the numerical optimisation of an appropriate model selection criterion, often
based on an estimator of generalisation performance, such as k-fold 
cross-validation.  The error of such an estimator can be broken down into bias 
and variance components.  While unbiasedness is often cited as a beneficial 
quality of a model selection criterion, we demonstrate that a low variance is 
at least as important, as a non-negligible variance introduces the potential 
for over-fitting in model selection as well as in training the model.  While 
this observation is in hindsight perhaps rather obvious, the degradation in 
performance due to over-fitting the model selection criterion can be 
surprisingly large, an observation that appears to have received little 
attention in the machine learning literature to date.  In this paper, we show 
that the effects of this form of over-fitting are often of comparable 
magnitude to differences in performance between learning algorithms, and thus 
cannot be ignored in empirical evaluation.  Furthermore, we show that some 
common performance evaluation practices are susceptible to a form of selection 
bias as a result of this form of over-fitting and hence are unreliable.  We 
discuss methods to avoid over-fitting in model selection and subsequent 
selection bias in performance evaluation, which we hope will be incorporated 
into best practice.  While this study concentrates on cross-validation based 
model selection, the findings are quite general and apply to any model 
selection practice involving the optimisation of a model selection criterion 
evaluated over a finite sample of data, including maximisation of the Bayesian 
evidence and optimisation of performance bounds.},
	number = {70},
	urldate = {2023-03-16},
	journal = {Journal of Machine Learning Research},
	author = {Cawley, Gavin C. and Talbot, Nicola L. C.},
	year = {2010},
	pages = {2079--2107},
}

@article{neunhoeffer_how_2019,
	title = {How {Cross}-{Validation} {Can} {Go} {Wrong} and {What} to {Do} {About} {It}},
	volume = {27},
	issn = {1047-1987, 1476-4989},
	url = {https://www.cambridge.org/core/product/identifier/S1047198718000396/type/journal_article},
	doi = {10.1017/pan.2018.39},
	language = {en},
	number = {1},
	urldate = {2023-03-16},
	journal = {Political Analysis},
	author = {Neunhoeffer, Marcel and Sternberg, Sebastian},
	month = jan,
	year = {2019},
	pages = {101--106},
}

@misc{vehtari_cross-validation_nodate,
	title = {Cross-validation {FAQ}},
	url = {https://avehtari.github.io/modelselection/CV-FAQ.html},
	journal = {Model selection tutorials and talks},
	author = {Vehtari, Aki},
}

@misc{noauthor_31_nodate,
	title = {3.1. {Cross}-{Validation}: {Evaluating} {Estimator} {Performance}},
	shorttitle = {3.1. {Cross}-validation},
	url = {https://scikit-learn/stable/modules/cross_validation.html},
	abstract = {Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would ha...},
	language = {en},
	urldate = {2023-03-16},
	journal = {scikit-learn},
}

@article{leist_mapping_2022,
	title = {Mapping of {Machine} {Learning} {Approaches} for {Description}, {Prediction}, and {Causal} {Inference} in the {Social} and {Health} {Sciences}},
	volume = {8},
	issn = {2375-2548},
	url = {https://www.science.org/doi/10.1126/sciadv.abk1942},
	doi = {10.1126/sciadv.abk1942},
	abstract = {Machine learning (ML) methodology used in the social and health sciences needs to fit the intended research purposes of description, prediction, or causal inference. This paper provides a comprehensive, systematic meta-mapping of research questions in the social and health sciences to appropriate ML approaches by incorporating the necessary requirements to statistical analysis in these disciplines. We map the established classification into description, prediction, counterfactual prediction, and causal structural learning to common research goals, such as estimating prevalence of adverse social or health outcomes, predicting the risk of an event, and identifying risk factors or causes of adverse outcomes, and explain common ML performance metrics. Such mapping may help to fully exploit the benefits of ML while considering domain-specific aspects relevant to the social and health sciences and hopefully contribute to the acceleration of the uptake of ML applications to advance both basic and applied social and health sciences research. 
          ,  
            ML methods are comprehensively reviewed for their optimal use in epidemiology, demography, psychology, health care, and economics.},
	language = {en},
	number = {42},
	urldate = {2023-03-16},
	journal = {Science Advances},
	author = {Leist, Anja K. and Klee, Matthias and Kim, Jung Hyun and Rehkopf, David H. and Bordas, Stéphane P. A. and Muniz-Terrera, Graciela and Wade, Sara},
	month = oct,
	year = {2022},
	pages = {eabk1942},
}

@article{shadbahr_classification_2022,
	title = {Classification of {Datasets} {With} {Imputed} {Missing} {Values}: {Does} {Imputation} {Quality} {Matter}?},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Classification of datasets with imputed missing values},
	url = {https://arxiv.org/abs/2206.08478},
	doi = {10.48550/ARXIV.2206.08478},
	abstract = {Classifying samples in incomplete datasets is a common aim for machine learning practitioners, but is non-trivial. Missing data is found in most real-world datasets and these missing values are typically imputed using established methods, followed by classification of the now complete, imputed, samples. The focus of the machine learning researcher is then to optimise the downstream classification performance. In this study, we highlight that it is imperative to consider the quality of the imputation. We demonstrate how the commonly used measures for assessing quality are flawed and propose a new class of discrepancy scores which focus on how well the method recreates the overall distribution of the data. To conclude, we highlight the compromised interpretability of classifier models trained using poorly imputed data.},
	urldate = {2023-03-16},
	author = {Shadbahr, Tolou and Roberts, Michael and Stanczuk, Jan and Gilbey, Julian and Teare, Philip and Dittmer, Sören and Thorpe, Matthew and Torne, Ramon Vinas and Sala, Evis and Lio, Pietro and Patel, Mishal and Collaboration, AIX-COVNET and Rudd, James H. F. and Mirtti, Tuomas and Rannikko, Antti and Aston, John A. D. and Tang, Jing and Schönlieb, Carola-Bibiane},
	year = {2022},
	keywords = {FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@article{vandenbroucke_strengthening_2007,
	title = {Strengthening the {Reporting} of {Observational} {Studies} in {Epidemiology} ({STROBE}): {Explanation} and {Elaboration}},
	volume = {4},
	issn = {1549-1676},
	shorttitle = {Strengthening the {Reporting} of {Observational} {Studies} in {Epidemiology} ({STROBE})},
	url = {https://dx.plos.org/10.1371/journal.pmed.0040297},
	doi = {10.1371/journal.pmed.0040297},
	language = {en},
	number = {10},
	urldate = {2023-03-16},
	journal = {PLoS Medicine},
	author = {Vandenbroucke, Jan P and Von Elm, Erik and Altman, Douglas G and Gøtzsche, Peter C and Mulrow, Cynthia D and Pocock, Stuart J and Poole, Charles and Schlesselman, James J and Egger, Matthias and {for the STROBE Initiative}},
	month = oct,
	year = {2007},
	pages = {e297},
}

@article{hofman_prediction_2017,
	title = {Prediction and {Explanation} in {Social} {Systems}},
	volume = {355},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.aal3856},
	doi = {10.1126/science.aal3856},
	abstract = {Historically, social scientists have sought out explanations of human and social phenomena that provide interpretable causal mechanisms, while often ignoring their predictive accuracy. We argue that the increasingly computational nature of social science is beginning to reverse this traditional bias against prediction; however, it has also highlighted three important issues that require resolution. First, current practices for evaluating predictions must be better standardized. Second, theoretical limits to predictive accuracy in complex social systems must be better characterized, thereby setting expectations for what can be predicted or explained. Third, predictive accuracy and interpretability must be recognized as complements, not substitutes, when evaluating explanations. Resolving these three issues will lead to better, more replicable, and more useful social science.},
	language = {en},
	number = {6324},
	urldate = {2023-03-16},
	journal = {Science},
	author = {Hofman, Jake M. and Sharma, Amit and Watts, Duncan J.},
	month = feb,
	year = {2017},
	pages = {486--488},
}

@misc{taherdoost_sampling_2016,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Sampling {Methods} in {Research} {Methodology}; {How} to {Choose} a {Sampling} {Technique} for {Research}},
	url = {https://papers.ssrn.com/abstract=3205035},
	doi = {10.2139/ssrn.3205035},
	abstract = {In order to answer the research questions, it is doubtful that researcher should be able to collect data from all cases. Thus, there is a need to select a sample. This paper presents the steps to go through to conduct sampling. Furthermore, as there are different types of sampling techniques/methods, researcher needs to understand the differences to select the proper sampling method for the research. In the regards, this paper also presents the different types of sampling techniques and methods.},
	language = {en},
	urldate = {2023-03-16},
	author = {Taherdoost, Hamed},
	month = apr,
	year = {2016},
	keywords = {Non-Probability Sampling, Probability Sampling, Research Methodology, Sampling Method, Sampling Technique},
}

@misc{noauthor_about_2023,
	title = {About {Brain} {Imaging} {Data} {Structure}},
	url = {https://bids.neuroimaging.io/index},
	abstract = {About BIDS Neuroimaging experiments result in complicated data that can be arranged in many different ways. So far there is no consensus how to organize and share data obtained in neuroimaging experiments. Even two researchers working in the same lab can opt to arrange their data in a different way. Lack of consensus (or a standard) leads to misunderstandings and time wasted on rearranging data or rewriting scripts expecting certain structure. With the Brain Imaging Data Structure (BIDS), we describe a simple and easy to adopt way of organizing neuroimaging and behavioral data. BIDS was heavily inspired by the format used internally by the OpenfMRI repository that is now known as OpenNeuro. While working on BIDS we consulted many neuroscientists to make sure it covers most common experiments, but at the same time is intuitive and easy to adopt. The specification is intentionally based on simple file formats and folder structures to reflect current lab practices and make it accessible to a wide range of scientists coming from different backgrounds. BIDS is a community effort BIDS is developed by the community for the community and everybody can become a part of the community. Specification vs. Ecosystem Since the inception of the BIDS specification that documents how to organize neuroimaging data, a large ecosystem of tools and resources has evolved around BIDS. A few of the key elements of this ecosystem are the BIDS-validator, to automatically check datasets for adherence to the specification, OpenNeuro, as a database for BIDS formatted datasets, and BIDS-Apps, a collection of portable neuroimaging pipelines that understand BIDS datasets. A non-exhaustive list of further tools can be found in the Benefits section. With the ongoing development of new tools and resources it is important to keep in mind that the BIDS specification remains the standard according to which the entire ecosystem must adhere. Further Information Good introductions to the BIDS standard can be found in the initial paper published in Nature Scientific Data, as well as in the follow up papers on specific modalities: MEG, EEG, and iEEG. Look through some of the community’s presentations on BIDS. We have constructed a grant writing kit to assist grant writers putting together BIDS related grant proposals. Take a look at how the community uses BIDS. We submitted an application to The Neuro Open Science in action prize 2020. Please find our associated application. Sign up to receive occasional updates by email and follow BIDS on Twitter.},
	language = {en-US},
	urldate = {2023-03-16},
	journal = {Brain Imaging Data Structure},
	month = mar,
	year = {2023},
}

@misc{chmielinski_dataset_2022,
	title = {The {Dataset} {Nutrition} {Label} (2nd {Gen}): {Leveraging} {Context} to {Mitigate} {Harms} in {Artificial} {Intelligence}},
	shorttitle = {The {Dataset} {Nutrition} {Label} (2nd {Gen})},
	url = {http://arxiv.org/abs/2201.03954},
	doi = {10.48550/arXiv.2201.03954},
	abstract = {As the production of and reliance on datasets to produce automated decision-making systems (ADS) increases, so does the need for processes for evaluating and interrogating the underlying data. After launching the Dataset Nutrition Label in 2018, the Data Nutrition Project has made significant updates to the design and purpose of the Label, and is launching an updated Label in late 2020, which is previewed in this paper. The new Label includes context-specific Use Cases \&Alerts presented through an updated design and user interface targeted towards the data scientist profile. This paper discusses the harm and bias from underlying training data that the Label is intended to mitigate, the current state of the work including new datasets being labeled, new and existing challenges, and further directions of the work, as well as Figures previewing the new label.},
	urldate = {2023-03-16},
	publisher = {arXiv},
	author = {Chmielinski, Kasia S. and Newman, Sarah and Taylor, Matt and Joseph, Josh and Thomas, Kemi and Yurkofsky, Jessica and Qiu, Yue Chelsea},
	month = mar,
	year = {2022},
	note = {arXiv:2201.03954 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@article{gebru_datasheets_2021,
	title = {Datasheets for {Datasets}},
	volume = {64},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3458723},
	doi = {10.1145/3458723},
	abstract = {Documentation to facilitate communication between dataset creators and consumers.},
	language = {en},
	number = {12},
	urldate = {2023-03-16},
	journal = {Communications of the ACM},
	author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Iii, Hal Daumé and Crawford, Kate},
	month = dec,
	year = {2021},
	pages = {86--92},
}

@article{paluck_contact_2019,
	title = {The {Contact} {Hypothesis} {Re}-{Evaluated}},
	volume = {3},
	issn = {2398-063X, 2398-0648},
	url = {https://www.cambridge.org/core/product/identifier/S2398063X18000258/type/journal_article},
	doi = {10.1017/bpp.2018.25},
	abstract = {Abstract 
            This paper evaluates the state of contact hypothesis research from a policy perspective. Building on Pettigrew and Tropp's (2006) influential meta-analysis, we assemble all intergroup contact studies that feature random assignment and delayed outcome measures, of which there are 27 in total, nearly two-thirds of which were published following the original review. We find the evidence from this updated dataset to be consistent with Pettigrew and Tropp's (2006) conclusion that contact “typically reduces prejudice.” At the same time, our meta-analysis suggests that contact's effects vary, with interventions directed at ethnic or racial prejudice generating substantially weaker effects. Moreover, our inventory of relevant studies reveals important gaps, most notably the absence of studies addressing adults' racial or ethnic prejudices, an important limitation for both theory and policy. We also call attention to the lack of research that systematically investigates the scope conditions suggested by Allport (1954) under which contact is most influential. We conclude that these gaps in contact research must be addressed empirically before this hypothesis can reliably guide policy.},
	language = {en},
	number = {02},
	urldate = {2023-03-16},
	journal = {Behavioural Public Policy},
	author = {Paluck, Elizabeth Levy and Green, Seth A. and Green, Donald P.},
	month = nov,
	year = {2019},
	pages = {129--158},
}

@misc{paluck_contact_2021,
	title = {The {Contact} {Hypothesis} {Re}-{Evaluated}: {Code} and {Data}},
	copyright = {MIT License, No Rights Reserved (CC0)},
	shorttitle = {The contact hypothesis re-evaluated},
	url = {https://codeocean.com/capsule/8235972/tree/v7},
	abstract = {This code reproduces the statistical analyses for 'The contact hypothesis re-evaluated,' by Betsy Levy Paluck, Seth Ariel Green, and Donald P. Green, available at https://doi.org/10.1017/bpp.2018.25 . This paper evaluates the state of contact hypothesis research from a policy perspective. Building on Pettigrew and Tropp’s (2006) influential meta-analysis, we assemble all intergroup contact studies that feature random assignment and delayed outcome measures, of which there are 27 in total, nearly two-thirds of which were published following the original review. We find the evidence from this updated dataset to be consistent with Pettigrew and Tropp’s (2006) conclusion that contact “typically reduces prejudice." At the same time, our meta-analysis suggests that contact’s effects vary, with interventions directed at ethnic or racial prejudice generating substantially weaker effects. Moreover, our inventory of relevant studies reveals important gaps, most notably the absence of studies addressing adults’ racial or ethnic prejudice, an important limitation for both theory and policy. We also call attention to the lack of research that systematically investigates the scope conditions suggested by Allport (1954) under which contact is most influential. We conclude that these gaps in contact research must be addressed empirically before this hypothesis can reliably guide policy.},
	urldate = {2023-03-16},
	publisher = {Code Ocean},
	author = {Paluck, Betsy Levy and Green, Seth Ariel and Green, Donald P.},
	collaborator = {Green, Seth},
	year = {2021},
	doi = {10.24433/CO.4024382.V7},
	keywords = {Capsule, contact-hypothesis, economics, literature-review, meta-analysis, policy, political-science, psychology, rstudio, Social Sciences, social-science},
}

@misc{comi_using_nodate,
	title = {Using {Codeocean} for {Sharing} {Reproducible} {Research} {\textbar} {The} {Princeton} {Research} {Software} {Engineering} {Group} {Blog}},
	url = {https://rse.princeton.edu/2021/03/using-codeocean-for-sharing-reproducible-research/},
	urldate = {2023-03-16},
	author = {Comi, Troy},
}

@misc{noauthor_how_nodate,
	title = {How {To} {Write} (and {Set}) a {Run} {Script} {\textbar} {Help} {\textbar} {Code} {Ocean}},
	url = {https://help.codeocean.com/en/articles/2465281-how-to-write-and-set-a-run-script},
	abstract = {Running your code from top to bottom by default},
	language = {en},
	urldate = {2023-03-16},
}

@misc{harbert_bash_2018,
	title = {Bash {Scripting}},
	url = {https://rsh249.github.io/bioinformatics/bash_script.html},
	urldate = {2023-03-16},
	author = {Harbert},
	month = oct,
	year = {2018},
}

@misc{singers_awesome_nodate,
	title = {Awesome {README}},
	url = {https://github.com/matiassingers/awesome-readme},
	journal = {GitHub},
	author = {Singers, Matias},
}

@article{vilhuber_lars_template_2020,
	title = {A {Template} {README} for {Social} {Science} {Replication} {Packages}},
	copyright = {Creative Commons Attribution Non Commercial 4.0 International, Open Access},
	url = {https://zenodo.org/record/4319999},
	doi = {10.5281/ZENODO.4319999},
	abstract = {The typical README in social science journals serves the purpose of guiding a reader through the available material and a route to replicating the results in the research paper, including the description of the origins of data and/or description of programs. As such, a good README file should first provide a brief overview of the available material and a brief guide as to how to proceed from beginning to end, before then diving into the specifics. These template files structure such a README in a way that is compliant with the typical data and code workflow in the social sciences.},
	language = {en},
	urldate = {2023-03-16},
	author = {{Vilhuber, Lars} and {Connolly, Marie} and {Koren, Miklós} and {Llull, Joan} and {Morrow, Peter}},
	month = dec,
	year = {2020},
	keywords = {economics, reproducibility, social sciences},
}

@misc{noauthor_nature_2017,
	title = {Nature {Research} {\textbar} {Code} and {Software} {Submission} {Checklist}},
	url = {https://www.nature.com/documents/nr-software-policy.pdf},
	urldate = {2023-03-16},
	publisher = {Nature Research},
	month = jun,
	year = {2017},
}

@article{stodden_best_2014,
	title = {Best {Practices} for {Computational} {Science}: {Software} {Infrastructure} and {Environments} for {Reproducible} and {Extensible} {Research}},
	volume = {2},
	shorttitle = {Best {Practices} for {Computational} {Science}},
	url = {https://openresearchsoftware.metajnl.com/articles/10.5334/jors.ay},
	doi = {10.5334/jors.ay},
	abstract = {The goal of this article is to coalesce a discussion around best practices for scholarly research that utilizes computational methods, by providing a formalized set of best practice recommendations to guide computational scientists and other stakeholders wishing to disseminate reproducible research, facilitate innovation by enabling data and code re-use, and enable broader communication of the output of computational scientific research. Scholarly dissemination and communication standards are changing to reflect the increasingly computational nature of scholarly research, primarily to include the sharing of the data and code associated with published results. We also present these Best Practices as a living, evolving, and changing document at http://wiki.stodden.net/Best\_Practices.},
	language = {en-US},
	number = {1},
	urldate = {2023-03-16},
	author = {Stodden, Victoria and Miguez, Sheila},
	month = jul,
	year = {2014},
	pages = {e21},
}

@misc{noauthor_requirements_nodate,
	title = {Requirements {File} {Format} - pip {Documentation} v23.0.1},
	url = {https://pip.pypa.io/en/stable/reference/requirements-file-format/},
	urldate = {2023-03-16},
}

@misc{noauthor_data_nodate,
	title = {Data {Dictionaries} {\textbar} {U}.{S}. {Geological} {Survey}},
	url = {https://www.usgs.gov/data-management/data-dictionaries},
	journal = {USGS},
}

@article{peng_mitigating_2021,
	title = {Mitigating {Dataset} {Harms} {Requires} {Stewardship}: {Lessons} {From} 1000 {Papers}},
	volume = {1},
	shorttitle = {Mitigating dataset harms requires stewardship},
	url = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/077e29b11be80ab57e1a2ecabb7da330-Abstract-round2.html},
	language = {en},
	urldate = {2023-03-16},
	journal = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
	author = {Peng, Kenneth and Mathur, Arunesh and Narayanan, Arvind},
	month = dec,
	year = {2021},
}

@misc{noauthor_unofficial_nodate,
	title = {Unofficial {Guidance} on {Various} {Topics} by {Social} {Science} {Data} {Editors}},
	url = {https://social-science-data-editors.github.io/guidance/},
	abstract = {Guidance for authors wishing to create data and code supplements, and for replicators.},
	language = {en-US},
	urldate = {2023-03-16},
	journal = {Data and Code Guidance by Data Editors},
}

@article{sandve_ten_2013,
	title = {Ten {Simple} {Rules} for {Reproducible} {Computational} {Research}},
	volume = {9},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1003285},
	doi = {10.1371/journal.pcbi.1003285},
	language = {en},
	number = {10},
	urldate = {2023-03-16},
	journal = {PLoS Computational Biology},
	author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
	editor = {Bourne, Philip E.},
	month = oct,
	year = {2013},
	pages = {e1003285},
}

@article{liu_successes_2019,
	title = {Successes and {Struggles} with {Computational} {Reproducibility}: {Lessons} from the {Fragile} {Families} {Challenge}},
	volume = {5},
	issn = {2378-0231, 2378-0231},
	shorttitle = {Successes and {Struggles} with {Computational} {Reproducibility}},
	url = {http://journals.sagepub.com/doi/10.1177/2378023119849803},
	doi = {10.1177/2378023119849803},
	abstract = {Reproducibility is fundamental to science, and an important component of reproducibility is computational reproducibility: the ability of a researcher to recreate the results of a published study using the original author’s raw data and code. Although most people agree that computational reproducibility is important, it is still difficult to achieve in practice. In this article, the authors describe their approach to enabling computational reproducibility for the 12 articles in this special issue of Socius about the Fragile Families Challenge. The approach draws on two tools commonly used by professional software engineers but not widely used by academic researchers: software containers (e.g., Docker) and cloud computing (e.g., Amazon Web Services). These tools made it possible to standardize the computing environment around each submission, which will ease computational reproducibility both today and in the future. Drawing on their successes and struggles, the authors conclude with recommendations to researchers and journals.},
	language = {en},
	urldate = {2023-03-16},
	journal = {Socius: Sociological Research for a Dynamic World},
	author = {Liu, David M. and Salganik, Matthew J.},
	month = jan,
	year = {2019},
	pages = {237802311984980},
}

@article{amrhein_scientists_2019-1,
	title = {Scientists {Rise} {Up} {Against} {Statistical} {Significance}},
	volume = {567},
	copyright = {2021 Nature},
	url = {https://www.nature.com/articles/d41586-019-00857-9},
	doi = {10.1038/d41586-019-00857-9},
	abstract = {Valentin Amrhein, Sander Greenland, Blake McShane and more than 800 signatories call for an end to hyped claims and the dismissal of possibly crucial effects.},
	language = {en},
	number = {7748},
	urldate = {2023-03-15},
	journal = {Nature},
	author = {Amrhein, Valentin and Greenland, Sander and McShane, Blake},
	month = mar,
	year = {2019},
	keywords = {Research data, Research management},
	pages = {305--307},
}

@misc{hullman_what_nodate,
	title = {What {Is} a {Claim} in {ML}-{Oriented} {Research}, and {When} {Is} {It} {Not} {Reproducible}? {\textbar} {Statistical} {Modeling}, {Causal} {Inference}, and {Social} {Science}},
	url = {https://statmodeling.stat.columbia.edu/2021/08/20/what-is-a-claim-in-ml-oriented-research-and-when-is-it-not-reproducible/},
	urldate = {2022-02-24},
	author = {Hullman, Jessica},
	file = {What is a claim in ML-oriented research, and when is it not reproducible? | Statistical Modeling, Causal Inference, and Social Science:C\:\\Users\\pthan\\Zotero\\storage\\X9CH5TED\\what-is-a-claim-in-ml-oriented-research-and-when-is-it-not-reproducible.html:text/html},
}

@article{collins_transparent_2015,
	title = {Transparent {Reporting} of a {Multivariable} {Prediction} {Model} for {Individual} {Prognosis} or {Diagnosis} ({TRIPOD}): {The} {TRIPOD} {Statement}},
	volume = {13},
	issn = {1741-7015},
	shorttitle = {Transparent reporting of a multivariable prediction model for individual prognosis or diagnosis ({TRIPOD})},
	url = {https://doi.org/10.1186/s12916-014-0241-z},
	doi = {10.1186/s12916-014-0241-z},
	abstract = {Prediction models are developed to aid health care providers in estimating the probability or risk that a specific disease or condition is present (diagnostic models) or that a specific event will occur in the future (prognostic models), to inform their decision making. However, the overwhelming evidence shows that the quality of reporting of prediction model studies is poor. Only with full and clear reporting of information on all aspects of a prediction model can risk of bias and potential usefulness of prediction models be adequately assessed. The Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD) Initiative developed a set of recommendations for the reporting of studies developing, validating, or updating a prediction model, whether for diagnostic or prognostic purposes. This article describes how the TRIPOD Statement was developed. An extensive list of items based on a review of the literature was created, which was reduced after a Web-based survey and revised during a 3-day meeting in June 2011 with methodologists, health care professionals, and journal editors. The list was refined during several meetings of the steering group and in e-mail discussions with the wider group of TRIPOD contributors. The resulting TRIPOD Statement is a checklist of 22 items, deemed essential for transparent reporting of a prediction model study. The TRIPOD Statement aims to improve the transparency of the reporting of a prediction model study regardless of the study methods used. The TRIPOD Statement is best used in conjunction with the TRIPOD explanation and elaboration document. To aid the editorial process and readers of prediction model studies, it is recommended that authors include a completed checklist in their submission (also available at www.tripod-statement.org).},
	number = {1},
	urldate = {2022-10-27},
	journal = {BMC Medicine},
	author = {Collins, Gary S. and Reitsma, Johannes B. and Altman, Douglas G. and Moons, Karel GM},
	month = jan,
	year = {2015},
	keywords = {Diagnostic, Model development, Prediction models, Prognostic, Reporting, Transparency, Validation},
	pages = {1},
}

@article{hullman_worst_2022-1,
	title = {The {Worst} of {Both} {Worlds}: {A} {Comparative} {Analysis} of {Errors} in {Learning} {From} {Data} in {Psychology} and {Machine} {Learning}},
	shorttitle = {The worst of both worlds},
	url = {http://arxiv.org/abs/2203.06498},
	abstract = {Recent concerns that machine learning (ML) may be facing a reproducibility and replication crisis suggest that some published claims in ML research cannot be taken at face value. These concerns inspire analogies to the replication crisis affecting the social and medical sciences, as well as calls for greater integration of statistical approaches to causal inference and predictive modeling. A deeper understanding of what reproducibility concerns in research in supervised ML have in common with the replication crisis in experimental science can put the new concerns in perspective, and help researchers avoid "the worst of both worlds" that can emerge when ML researchers begin borrowing methodologies from explanatory modeling without understanding their limitations, and vice versa. We contribute a comparative analysis of concerns about inductive learning that arise in different stages of the modeling pipeline in causal attribution as exemplified in psychology versus predictive modeling as exemplified by ML. We identify themes that re-occur in reform discussions like overreliance on asymptotic theory and non-credible beliefs about real-world data generating processes. We argue that in both fields, claims from learning are implied to generalize outside the specific environment studied (e.g., the input dataset or subject sample, modeling implementation, etc.) but are often impossible to refute due to forms of underspecification. In particular, many errors being acknowledged in ML expose cracks in long-held beliefs that optimizing predictive accuracy using huge datasets absolves one from having to make assumptions about the underlying data generating process. We conclude by discussing rhetorical risks like error misdiagnosis that arise in times of methodological uncertainty.},
	urldate = {2022-03-23},
	journal = {arXiv:2203.06498 [cs]},
	author = {Hullman, Jessica and Kapoor, Sayash and Nanayakkara, Priyanka and Gelman, Andrew and Narayanan, Arvind},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.06498},
	keywords = {Computer Science - Machine Learning},
}

@article{macleod_mdar_2021,
	title = {The {MDAR} ({Materials} {Design} {Analysis} {Reporting}) {Framework} for {Transparent} {Reporting} in the {Life} {Sciences}},
	volume = {118},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.2103238118},
	doi = {10.1073/pnas.2103238118},
	language = {en},
	number = {17},
	urldate = {2022-11-02},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Macleod, Malcolm and Collings, Andrew M. and Graf, Chris and Kiermer, Veronique and Mellor, David and Swaminathan, Sowmya and Sweet, Deborah and Vinson, Valda},
	month = apr,
	year = {2021},
	pages = {e2103238118},
}

@article{bossuyt_stard_2015,
	title = {{STARD} 2015: {An} {Updated} {List} of {Essential} {Items} for {Reporting} {Diagnostic} {Accuracy} {Studies}},
	issn = {1756-1833},
	shorttitle = {{STARD} 2015},
	url = {https://www.bmj.com/lookup/doi/10.1136/bmj.h5527},
	doi = {10.1136/bmj.h5527},
	language = {en},
	urldate = {2022-10-27},
	journal = {BMJ},
	author = {Bossuyt, Patrick M and Reitsma, Johannes B and Bruns, David E and Gatsonis, Constantine A and Glasziou, Paul P and Irwig, Les and Lijmer, Jeroen G and Moher, David and Rennie, Drummond and de Vet, Henrica C W and Kressel, Herbert Y and Rifai, Nader and Golub, Robert M and Altman, Douglas G and Hooft, Lotty and Korevaar, Daniël A and Cohen, Jérémie F},
	month = oct,
	year = {2015},
	pages = {h5527},
}

@misc{science_science_nodate,
	title = {Science {Journals}: {Editorial} {Policies}},
	shorttitle = {{\textless}em{\textgreater}{Science}{\textless}/em{\textgreater} {Journals}},
	url = {https://www.science.org/content/page/science-journals-editorial-policies},
	language = {en},
	urldate = {2022-11-02},
	author = {{Science}},
}

@article{mcdermott_reproducibility_2021,
	title = {Reproducibility in {Machine} {Learning} for {Health} {Research}: {Still} a {Ways} {To} {Go}},
	volume = {13},
	copyright = {Copyright © 2021 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. https://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
	issn = {1946-6234, 1946-6242},
	shorttitle = {Reproducibility in machine learning for health research},
	url = {https://stm.sciencemag.org/content/13/586/eabb1655},
	doi = {10.1126/scitranslmed.abb1655},
	abstract = {{\textless}p{\textgreater}Machine learning for health must be reproducible to ensure reliable clinical use. We evaluated 511 scientific papers across several machine learning subfields and found that machine learning for health compared poorly to other areas regarding reproducibility metrics, such as dataset and code accessibility. We propose recommendations to address this problem.{\textless}/p{\textgreater}},
	language = {en},
	number = {586},
	urldate = {2021-06-23},
	journal = {Science Translational Medicine},
	author = {McDermott, Matthew B. A. and Wang, Shirly and Marinsek, Nikki and Ranganath, Rajesh and Foschini, Luca and Ghassemi, Marzyeh},
	month = mar,
	year = {2021},
	pmid = {33762434},
	note = {Publisher: American Association for the Advancement of Science
Section: Perspective},
	file = {Snapshot:C\:\\Users\\pthan\\Zotero\\storage\\5KUTNN43\\eabb1655.html:text/html},
}

@misc{nature_reporting_nodate,
	title = {Reporting {Standards} and {Availability} of {Data}, {Materials}, {Code} and {Protocols}},
	url = {https://www.nih.gov/research-training/rigor-reproducibility/principles-guidelines-reporting-preclinical-research},
	abstract = {Rigorous statistical analysis, transparency in reporting, data and material sharing, consideration of refutations, and consider establishing best practice guidelines.},
	language = {EN},
	urldate = {2022-10-27},
	journal = {Nature},
	author = {{Nature}},
}

@misc{noauthor_reporting_nodate,
	title = {Reporting {Guidelines} {\textbar} {The} {EQUATOR} {Network}},
	url = {https://www.equator-network.org/reporting-guidelines/},
	urldate = {2022-10-27},
}

@article{hofman_prediction_2017-1,
	title = {Prediction and {Explanation} in {Social} {Systems}},
	volume = {355},
	copyright = {Copyright © 2017, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/355/6324/486},
	doi = {10.1126/science.aal3856},
	abstract = {Historically, social scientists have sought out explanations of human and social phenomena that provide interpretable causal mechanisms, while often ignoring their predictive accuracy. We argue that the increasingly computational nature of social science is beginning to reverse this traditional bias against prediction; however, it has also highlighted three important issues that require resolution. First, current practices for evaluating predictions must be better standardized. Second, theoretical limits to predictive accuracy in complex social systems must be better characterized, thereby setting expectations for what can be predicted or explained. Third, predictive accuracy and interpretability must be recognized as complements, not substitutes, when evaluating explanations. Resolving these three issues will lead to better, more replicable, and more useful social science.},
	language = {en},
	number = {6324},
	urldate = {2021-05-16},
	journal = {Science},
	author = {Hofman, Jake M. and Sharma, Amit and Watts, Duncan J.},
	month = feb,
	year = {2017},
	pmid = {28154051},
	note = {Publisher: American Association for the Advancement of Science
Section: Essays},
	pages = {486--488},
	file = {Snapshot:C\:\\Users\\pthan\\Zotero\\storage\\BJ6MADCW\\tab-pdf.html:text/html},
}

@article{vandewiele_overly_2021-1,
	title = {Overly {Optimistic} {Prediction} {Results} on {Imbalanced} {Data}: {A} {Case} {Study} of {Flaws} and {Benefits} {When} {Applying} {Over}-{Sampling}},
	volume = {111},
	issn = {0933-3657},
	shorttitle = {Overly optimistic prediction results on imbalanced data},
	url = {https://www.sciencedirect.com/science/article/pii/S0933365720312525},
	doi = {10.1016/j.artmed.2020.101987},
	abstract = {Information extracted from electrohysterography recordings could potentially prove to be an interesting additional source of information to estimate the risk on preterm birth. Recently, a large number of studies have reported near-perfect results to distinguish between recordings of patients that will deliver term or preterm using a public resource, called the Term/Preterm Electrohysterogram database. However, we argue that these results are overly optimistic due to a methodological flaw being made. In this work, we focus on one specific type of methodological flaw: applying over-sampling before partitioning the data into mutually exclusive training and testing sets. We show how this causes the results to be biased using two artificial datasets and reproduce results of studies in which this flaw was identified. Moreover, we evaluate the actual impact of over-sampling on predictive performance, when applied prior to data partitioning, using the same methodologies of related studies, to provide a realistic view of these methodologies’ generalization capabilities. We make our research reproducible by providing all the code under an open license.},
	language = {en},
	urldate = {2021-05-12},
	journal = {Artificial Intelligence in Medicine},
	author = {Vandewiele, Gilles and Dehaene, Isabelle and Kovács, György and Sterckx, Lucas and Janssens, Olivier and Ongenae, Femke and De Backere, Femke and De Turck, Filip and Roelens, Kristien and Decruyenaere, Johan and Van Hoecke, Sofie and Demeester, Thomas},
	month = jan,
	year = {2021},
	keywords = {Electrohysterography, Over-sampling, Preterm birth risk estimation},
	pages = {101987},
	file = {ScienceDirect Snapshot:C\:\\Users\\pthan\\Zotero\\storage\\4AFDZGM8\\S0933365720312525.html:text/html},
}

@article{serra-garcia_nonreplicable_2021,
	title = {Nonreplicable {Publications} {Are} {Cited} {More} {Than} {Replicable} {Ones}},
	volume = {7},
	issn = {2375-2548},
	url = {https://www.science.org/doi/10.1126/sciadv.abd1705},
	doi = {10.1126/sciadv.abd1705},
	abstract = {Published papers that fail to replicate are cited more than those that replicate, even after the failure is published. 
          ,  
            We use publicly available data to show that published papers in top psychology, economics, and general interest journals that fail to replicate are cited more than those that replicate. This difference in citation does not change after the publication of the failure to replicate. Only 12\% of postreplication citations of nonreplicable findings acknowledge the replication failure. Existing evidence also shows that experts predict well which papers will be replicated. Given this prediction, why are nonreplicable papers accepted for publication in the first place? A possible answer is that the review team faces a trade-off. When the results are more “interesting,” they apply lower standards regarding their reproducibility.},
	language = {en},
	number = {21},
	urldate = {2022-11-02},
	journal = {Science Advances},
	author = {Serra-Garcia, Marta and Gneezy, Uri},
	month = may,
	year = {2021},
	pages = {eabd1705},
}

@misc{dittmer_navigating_2022,
	title = {Navigating the {Challenges} in {Creating} {Complex} {Data} {Systems}: {A} {Development} {Philosophy}},
	shorttitle = {Navigating the challenges in creating complex data systems},
	url = {http://arxiv.org/abs/2210.13191},
	doi = {10.48550/arXiv.2210.13191},
	abstract = {In this perspective, we argue that despite the democratization of powerful tools for data science and machine learning over the last decade, developing the code for a trustworthy and effective data science system (DSS) is getting harder. Perverse incentives and a lack of widespread software engineering (SE) skills are among many root causes we identify that naturally give rise to the current systemic crisis in reproducibility of DSSs. We analyze why SE and building large complex systems is, in general, hard. Based on these insights, we identify how SE addresses those difficulties and how we can apply and generalize SE methods to construct DSSs that are fit for purpose. We advocate two key development philosophies, namely that one should incrementally grow -- not biphasically plan and build -- DSSs, and one should always employ two types of feedback loops during development: one which tests the code's correctness and another that evaluates the code's efficacy.},
	urldate = {2023-02-27},
	publisher = {arXiv},
	author = {Dittmer, Sören and Roberts, Michael and Gilbey, Julian and Biguri, Ander and Collaboration, AIX-COVNET and Preller, Jacobus and Rudd, James H. F. and Aston, John A. D. and Schönlieb, Carola-Bibiane},
	month = oct,
	year = {2022},
	note = {arXiv:2210.13191 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@article{gurevitch_meta-analysis_2018,
	title = {Meta-{Analysis} and the {Science} of {Research} {Synthesis}},
	volume = {555},
	copyright = {2018 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature25753},
	doi = {10.1038/nature25753},
	abstract = {Meta-analysis is the quantitative, scientific synthesis of research results. Since the term and modern approaches to research synthesis were first introduced in the 1970s, meta-analysis has had a revolutionary effect in many scientific fields, helping to establish evidence-based practice and to resolve seemingly contradictory research outcomes. At the same time, its implementation has engendered criticism and controversy, in some cases general and others specific to particular disciplines. Here we take the opportunity provided by the recent fortieth anniversary of meta-analysis to reflect on the accomplishments, limitations, recent advances and directions for future developments in the field of research synthesis.},
	language = {en},
	number = {7695},
	urldate = {2022-01-14},
	journal = {Nature},
	author = {Gurevitch, Jessica and Koricheva, Julia and Nakagawa, Shinichi and Stewart, Gavin},
	month = mar,
	year = {2018},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 7695
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Biodiversity;Outcomes research
Subject\_term\_id: biodiversity;outcomes-research},
	keywords = {Biodiversity, Outcomes research},
	pages = {175--182},
	file = {Snapshot:C\:\\Users\\pthan\\Zotero\\storage\\XU9DZ7N4\\nature25753.html:text/html},
}

@article{salganik_measuring_2020,
	title = {Measuring the {Predictability} of {Life} {Outcomes} {With} a {Scientific} {Mass} {Collaboration}},
	volume = {117},
	copyright = {Copyright © 2020 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/117/15/8398},
	doi = {10.1073/pnas.1915006117},
	abstract = {How predictable are life trajectories? We investigated this question with a scientific mass collaboration using the common task method; 160 teams built predictive models for six life outcomes using data from the Fragile Families and Child Wellbeing Study, a high-quality birth cohort study. Despite using a rich dataset and applying machine-learning methods optimized for prediction, the best predictions were not very accurate and were only slightly better than those from a simple benchmark model. Within each outcome, prediction error was strongly associated with the family being predicted and weakly associated with the technique used to generate the prediction. Overall, these results suggest practical limits to the predictability of life outcomes in some settings and illustrate the value of mass collaborations in the social sciences.},
	language = {en},
	number = {15},
	urldate = {2021-05-16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Salganik, Matthew J. and Lundberg, Ian and Kindel, Alexander T. and Ahearn, Caitlin E. and Al-Ghoneim, Khaled and Almaatouq, Abdullah and Altschul, Drew M. and Brand, Jennie E. and Carnegie, Nicole Bohme and Compton, Ryan James and Datta, Debanjan and Davidson, Thomas and Filippova, Anna and Gilroy, Connor and Goode, Brian J. and Jahani, Eaman and Kashyap, Ridhi and Kirchner, Antje and McKay, Stephen and Morgan, Allison C. and Pentland, Alex and Polimis, Kivan and Raes, Louis and Rigobon, Daniel E. and Roberts, Claudia V. and Stanescu, Diana M. and Suhara, Yoshihiko and Usmani, Adaner and Wang, Erik H. and Adem, Muna and Alhajri, Abdulla and AlShebli, Bedoor and Amin, Redwane and Amos, Ryan B. and Argyle, Lisa P. and Baer-Bositis, Livia and Büchi, Moritz and Chung, Bo-Ryehn and Eggert, William and Faletto, Gregory and Fan, Zhilin and Freese, Jeremy and Gadgil, Tejomay and Gagné, Josh and Gao, Yue and Halpern-Manners, Andrew and Hashim, Sonia P. and Hausen, Sonia and He, Guanhua and Higuera, Kimberly and Hogan, Bernie and Horwitz, Ilana M. and Hummel, Lisa M. and Jain, Naman and Jin, Kun and Jurgens, David and Kaminski, Patrick and Karapetyan, Areg and Kim, E. H. and Leizman, Ben and Liu, Naijia and Möser, Malte and Mack, Andrew E. and Mahajan, Mayank and Mandell, Noah and Marahrens, Helge and Mercado-Garcia, Diana and Mocz, Viola and Mueller-Gastell, Katariina and Musse, Ahmed and Niu, Qiankun and Nowak, William and Omidvar, Hamidreza and Or, Andrew and Ouyang, Karen and Pinto, Katy M. and Porter, Ethan and Porter, Kristin E. and Qian, Crystal and Rauf, Tamkinat and Sargsyan, Anahit and Schaffner, Thomas and Schnabel, Landon and Schonfeld, Bryan and Sender, Ben and Tang, Jonathan D. and Tsurkov, Emma and Loon, Austin van and Varol, Onur and Wang, Xiafei and Wang, Zhi and Wang, Julia and Wang, Flora and Weissman, Samantha and Whitaker, Kirstie and Wolters, Maria K. and Woon, Wei Lee and Wu, James and Wu, Catherine and Yang, Kengran and Yin, Jingwen and Zhao, Bingyu and Zhu, Chenyun and Brooks-Gunn, Jeanne and Engelhardt, Barbara E. and Hardt, Moritz and Knox, Dean and Levy, Karen and Narayanan, Arvind and Stewart, Brandon M. and Watts, Duncan J. and McLanahan, Sara},
	month = apr,
	year = {2020},
	pmid = {32229555},
	note = {Publisher: National Academy of Sciences
Section: Social Sciences},
	keywords = {machine learning, prediction, life course, mass collaboration},
	pages = {8398--8403},
	file = {Snapshot:C\:\\Users\\pthan\\Zotero\\storage\\GQM5HXJ8\\8398.html:text/html},
}

@article{demasi_meaningless_2017,
	title = {Meaningless {Comparisons} {Lead} to {False} {Optimism} in {Medical} {Machine} {Learning}},
	volume = {12},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0184604},
	doi = {10.1371/journal.pone.0184604},
	abstract = {A new trend in medicine is the use of algorithms to analyze big datasets, e.g. using everything your phone measures about you for diagnostics or monitoring. However, these algorithms are commonly compared against weak baselines, which may contribute to excessive optimism. To assess how well an algorithm works, scientists typically ask how well its output correlates with medically assigned scores. Here we perform a meta-analysis to quantify how the literature evaluates their algorithms for monitoring mental wellbeing. We find that the bulk of the literature (∼77\%) uses meaningless comparisons that ignore patient baseline state. For example, having an algorithm that uses phone data to diagnose mood disorders would be useful. However, it is possible to explain over 80\% of the variance of some mood measures in the population by simply guessing that each patient has their own average mood—the patient-specific baseline. Thus, an algorithm that just predicts that our mood is like it usually is can explain the majority of variance, but is, obviously, entirely useless. Comparing to the wrong (population) baseline has a massive effect on the perceived quality of algorithms and produces baseless optimism in the field. To solve this problem we propose “user lift” that reduces these systematic errors in the evaluation of personalized medical monitoring.},
	language = {en},
	number = {9},
	urldate = {2022-10-27},
	journal = {PLOS ONE},
	author = {DeMasi, Orianna and Kording, Konrad and Recht, Benjamin},
	month = sep,
	year = {2017},
	keywords = {Emotions, Cell phones, Diagnostic medicine, Happiness, Machine learning, Machine learning algorithms, Permutation, Psychological stress},
	pages = {e0184604},
}

@article{leist_mapping_2022-1,
	title = {Mapping of {Machine} {Learning} {Approaches} for {Description}, {Prediction}, and {Causal} {Inference} in the {Social} and {Health} {Sciences}},
	volume = {8},
	issn = {2375-2548},
	url = {https://www.science.org/doi/10.1126/sciadv.abk1942},
	doi = {10.1126/sciadv.abk1942},
	abstract = {Machine learning (ML) methodology used in the social and health sciences needs to fit the intended research purposes of description, prediction, or causal inference. This paper provides a comprehensive, systematic meta-mapping of research questions in the social and health sciences to appropriate ML approaches by incorporating the necessary requirements to statistical analysis in these disciplines. We map the established classification into description, prediction, counterfactual prediction, and causal structural learning to common research goals, such as estimating prevalence of adverse social or health outcomes, predicting the risk of an event, and identifying risk factors or causes of adverse outcomes, and explain common ML performance metrics. Such mapping may help to fully exploit the benefits of ML while considering domain-specific aspects relevant to the social and health sciences and hopefully contribute to the acceleration of the uptake of ML applications to advance both basic and applied social and health sciences research. 
          ,  
            ML methods are comprehensively reviewed for their optimal use in epidemiology, demography, psychology, health care, and economics.},
	language = {en},
	number = {42},
	urldate = {2022-10-27},
	journal = {Science Advances},
	author = {Leist, Anja K. and Klee, Matthias and Kim, Jung Hyun and Rehkopf, David H. and Bordas, Stéphane P. A. and Muniz-Terrera, Graciela and Wade, Sara},
	month = oct,
	year = {2022},
	pages = {eabk1942},
}

@article{iniesta_machine_2016,
	title = {Machine {Learning}, {Statistical} {Learning} and the {Future} of {Biological} {Research} in {Psychiatry}},
	volume = {46},
	issn = {0033-2917, 1469-8978},
	url = {https://www.cambridge.org/core/journals/psychological-medicine/article/machine-learning-statistical-learning-and-the-future-of-biological-research-in-psychiatry/7A6B51FCC58B10BDB8B51178AD52B50D},
	doi = {10.1017/S0033291716001367},
	abstract = {Psychiatric research has entered the age of ‘Big Data’. Datasets now routinely involve thousands of heterogeneous variables, including clinical, neuroimaging, genomic, proteomic, transcriptomic and other ‘omic’ measures. The analysis of these datasets is challenging, especially when the number of measurements exceeds the number of individuals, and may be further complicated by missing data for some subjects and variables that are highly correlated. Statistical learning-based models are a natural extension of classical statistical approaches but provide more effective methods to analyse very large datasets. In addition, the predictive capability of such models promises to be useful in developing decision support systems. That is, methods that can be introduced to clinical settings and guide, for example, diagnosis classification or personalized treatment. In this review, we aim to outline the potential benefits of statistical learning methods in clinical research. We first introduce the concept of Big Data in different environments. We then describe how modern statistical learning models can be used in practice on Big Datasets to extract relevant information. Finally, we discuss the strengths of using statistical learning in psychiatric studies, from both research and practical clinical points of view.},
	language = {en},
	number = {12},
	urldate = {2021-05-13},
	journal = {Psychological Medicine},
	author = {Iniesta, R. and Stahl, D. and McGuffin, P.},
	month = sep,
	year = {2016},
	note = {Publisher: Cambridge University Press},
	keywords = {Machine learning, outcome prediction, personalized medicine, predictive modelling, statistical learning},
	pages = {2455--2465},
	file = {Snapshot:C\:\\Users\\pthan\\Zotero\\storage\\ATJ83RJT\\7A6B51FCC58B10BDB8B51178AD52B50D.html:text/html},
}

@article{varoquaux_machine_2022,
	title = {Machine {Learning} for {Medical} {Imaging}: {Methodological} {Failures} and {Recommendations} for the {Future}},
	volume = {5},
	issn = {2398-6352},
	shorttitle = {Machine learning for medical imaging},
	doi = {10.1038/s41746-022-00592-y},
	abstract = {Research in computer analysis of medical images bears many promises to improve patients' health. However, a number of systematic challenges are slowing down the progress of the field, from limitations of the data, such as biases, to research incentives, such as optimizing for publication. In this paper we review roadblocks to developing and assessing methods. Building our analysis on evidence from the literature and data challenges, we show that at every step, potential biases can creep in. On a positive note, we also discuss on-going efforts to counteract these problems. Finally we provide recommendations on how to further address these problems in the future.},
	language = {eng},
	number = {1},
	journal = {NPJ digital medicine},
	author = {Varoquaux, Gaël and Cheplygina, Veronika},
	month = apr,
	year = {2022},
	pmid = {35413988},
	pmcid = {PMC9005663},
	pages = {48},
}

@article{kaufman_leakage_2012,
	title = {Leakage in {Data} {Mining}: {Formulation}, {Detection}, and {Avoidance}},
	volume = {6},
	issn = {1556-4681},
	shorttitle = {Leakage in data mining},
	url = {https://doi.org/10.1145/2382577.2382579},
	doi = {10.1145/2382577.2382579},
	abstract = {Deemed “one of the top ten data mining mistakes”, leakage is the introduction of information about the data mining target that should not be legitimately available to mine from. In addition to our own industry experience with real-life projects, controversies around several major public data mining competitions held recently such as the INFORMS 2010 Data Mining Challenge and the IJCNN 2011 Social Network Challenge are evidence that this issue is as relevant today as it has ever been. While acknowledging the importance and prevalence of leakage in both synthetic competitions and real-life data mining projects, existing literature has largely left this idea unexplored. What little has been said turns out not to be broad enough to cover more complex cases of leakage, such as those where the classical independently and identically distributed (i.i.d.) assumption is violated, that have been recently documented. In our new approach, these cases and others are explained by explicitly defining modeling goals and analyzing the broader framework of the data mining problem. The resulting definition enables us to derive general methodology for dealing with the issue. We show that it is possible to avoid leakage with a simple specific approach to data management followed by what we call a learn-predict separation, and present several ways of detecting leakage when the modeler has no control over how the data have been collected. We also offer an alternative point of view on leakage that is based on causal graph modeling concepts.},
	number = {4},
	urldate = {2023-02-27},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Kaufman, Shachar and Rosset, Saharon and Perlich, Claudia and Stitelman, Ori},
	month = dec,
	year = {2012},
	keywords = {Data mining, leakage, predictive modeling},
	pages = {15:1--15:21},
}

@article{hofman_integrating_2021,
	title = {Integrating {Explanation} and {Prediction} in {Computational} {Social} {Science}},
	volume = {595},
	copyright = {2021 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03659-0},
	doi = {10.1038/s41586-021-03659-0},
	abstract = {Computational social science is more than just large repositories of digital data and the computational methods needed to construct and analyse them. It also represents a convergence of different fields with different ways of thinking about and doing science. The goal of this Perspective is to provide some clarity around how these approaches differ from one another and to propose how they might be productively integrated. Towards this end we make two contributions. The first is a schema for thinking about research activities along two dimensions—the extent to which work is explanatory, focusing on identifying and estimating causal effects, and the degree of consideration given to testing predictions of outcomes—and how these two priorities can complement, rather than compete with, one another. Our second contribution is to advocate that computational social scientists devote more attention to combining prediction and explanation, which we call integrative modelling, and to outline some practical suggestions for realizing this goal.},
	language = {en},
	number = {7866},
	urldate = {2021-07-16},
	journal = {Nature},
	author = {Hofman, Jake M. and Watts, Duncan J. and Athey, Susan and Garip, Filiz and Griffiths, Thomas L. and Kleinberg, Jon and Margetts, Helen and Mullainathan, Sendhil and Salganik, Matthew J. and Vazire, Simine and Vespignani, Alessandro and Yarkoni, Tal},
	month = jul,
	year = {2021},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 7866
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Interdisciplinary studies;Scientific community
Subject\_term\_id: interdisciplinary-studies;scientific-community},
	pages = {181--188},
	file = {Snapshot:C\:\\Users\\pthan\\Zotero\\storage\\6UJDG6WH\\s41586-021-03659-0.html:text/html},
}

@article{pineau_improving_2022,
	title = {Improving {Reproducibility} in {Machine} {Learning} {Research} (a {Report} {From} the {NeurIPS} 2019 {Reproducibility} {Program})},
	volume = {22},
	issn = {1532-4435},
	abstract = {One of the challenges in machine learning research is to ensure that presented and published results are sound and reliable. Reproducibility, that is obtaining similar results as presented in a paper or talk, using the same code and data (when available), is a necessary step to verify the reliability of research findings. Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice. Reproducibility also promotes the use of robust experimental work ows, which potentially reduce unintentional errors. In 2019, the Neural Information Processing Systems (NeurIPS) conference, the premier international conference for research in machine learning, introduced a reproducibility program, designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research. The program contained three components: a code submission policy, a community-wide reproducibility challenge, and the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process. In this paper, we describe each of these components, how it was deployed, as well as what we were able to learn from this initiative.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Pineau, Joelle and Vincent-Lamarre, Philippe and Sinha, Koustuv and Larivière, Vincent and Beygelzimer, Alina and d'Alché-Buc, Florence and Fox, Emily and Larochelle, Hugo},
	month = jul,
	year = {2022},
	keywords = {reproducibility, NeurIPS 2019},
	pages = {164:7459--164:7478},
}

@article{simmonds_how_2022-1,
	title = {How {Is} {Model}-{Related} {Uncertainty} {Quantified} and {Reported} in {Different} {Disciplines}?},
	url = {http://arxiv.org/abs/2206.12179},
	doi = {10.48550/arXiv.2206.12179},
	abstract = {How do we know how much we know? Quantifying uncertainty associated with our modelling work is the only way we can answer how much we know about any phenomenon. With quantitative science now highly influential in the public sphere and the results from models translating into action, we must support our conclusions with sufficient rigour to produce useful, reproducible results. Incomplete consideration of model-based uncertainties can lead to false conclusions with real world impacts. Despite these potentially damaging consequences, uncertainty consideration is incomplete both within and across scientific fields. We take a unique interdisciplinary approach and conduct a systematic audit of model-related uncertainty quantification from seven scientific fields, spanning the biological, physical, and social sciences. Our results show no single field is achieving complete consideration of model uncertainties, but together we can fill the gaps. We propose opportunities to improve the quantification of uncertainty through use of a source framework for uncertainty consideration, model type specific guidelines, improved presentation, and shared best practice. We also identify shared outstanding challenges (uncertainty in input data, balancing trade-offs, error propagation, and defining how much uncertainty is required). Finally, we make nine concrete recommendations for current practice (following good practice guidelines and an uncertainty checklist, presenting uncertainty numerically, and propagating model-related uncertainty into conclusions), future research priorities (uncertainty in input data, quantifying uncertainty in complex models, and the importance of missing uncertainty in different contexts), and general research standards across the sciences (transparency about study limitations and dedicated uncertainty sections of manuscripts).},
	urldate = {2022-11-07},
	author = {Simmonds, Emily G. and Adjei, Kwaku Peprah and Andersen, Christoffer Wold and Aspheim, Janne Cathrin Hetle and Battistin, Claudia and Bulso, Nicola and Christensen, Hannah and Cretois, Benjamin and Cubero, Ryan and Davidovich, Ivan A. and Dickel, Lisa and Dunn, Benjamin and Dunn-Sigouin, Etienne and Dyrstad, Karin and Einum, Sigurd and Giglio, Donata and Gjerlow, Haakon and Godefroidt, Amelie and Gonzalez-Gil, Ricardo and Cogno, Soledad Gonzalo and Grosse, Fabian and Halloran, Paul and Jensen, Mari F. and Kennedy, John James and Langsaether, Peter Egge and Laverick, Jack H. and Lederberger, Debora and Li, Camille and Mandeville, Elizabeth and Mandeville, Caitlin and Moe, Espen and Schroder, Tobias Navarro and Nunan, David and Parada, Jorge Sicacha and Simpson, Melanie Rae and Skarstein, Emma Sofie and Spensberger, Clemens and Stevens, Richard and Subramanian, Aneesh and Svendsen, Lea and Theisen, Ole Magnus and Watret, Connor and OHara, Robert B.},
	year = {2022},
	note = {arXiv:2206.12179 [physics, q-bio, stat]},
	keywords = {Physics - Atmospheric and Oceanic Physics, Quantitative Biology - Quantitative Methods, Statistics - Applications},
}

@article{hofman_expanding_2021,
	title = {Expanding the {Scope} of {Reproducibility} {Research} {Through} {Data} {Analysis} {Replications}},
	volume = {164},
	issn = {0749-5978},
	url = {https://www.sciencedirect.com/science/article/pii/S0749597820304076},
	doi = {10.1016/j.obhdp.2020.11.003},
	abstract = {In recent years, researchers in several scientific disciplines have become concerned with published studies replicating less often than expected. A positive side effect of this concern is an appreciation that replicating other researchers’ work is an essential part of the scientific process. To date, many such efforts have come from the experimental sciences, where replication entails running new experiments, generating new data, and analyzing it. In this article, we emphasize not experimental replications but data analysis replications. We do so for three reasons. First, experimental replication excludes entire classes of publications that do not run experiments or even collect original data (e.g., archival data analysis). Second, experimental replication may in some cases be a needlessly high bar: there is great value in replicating just the data analyses of published experimental work. As data analysis replications require a lower investment of resources than experimental replications, their adoption should expand the number and variety of scientific reproducibility studies undertaken. Third, we propose that teaching undergraduate students to perform data analysis replications will greatly increase the number of replications done while providing them with research experience that should inform their decisions to pursue research or to attend graduate school. Towards this end, we provide details of a pilot program we created to teach undergraduates the skills necessary to conduct data analysis replications, and include a case study of the first set of students who completed this program and attempted to replicate the data analyses in a widely-cited social science paper on policing. In addition, we present a summary of ten additional data analysis replications carried out entirely by students in a university course.},
	language = {en},
	urldate = {2022-07-13},
	journal = {Organizational Behavior and Human Decision Processes},
	author = {Hofman, Jake M. and Goldstein, Daniel G. and Sen, Siddhartha and Poursabzi-Sangdeh, Forough and Allen, Jennifer and Dong, Ling Liang and Fried, Brenda and Gaur, Harpreet and Hoq, Adnan and Mbazor, Emeka and Moreira, Naomi and Muso, Cindy and Rapp, Etta and Terrero, Roymil},
	month = may,
	year = {2021},
	keywords = {Education, Data analysis, Replication, Reproducibility, Robustness},
	pages = {192--202},
}

@article{plint_does_2006,
	title = {Does the {CONSORT} {Checklist} {Improve} the {Quality} of {Reports} of {Randomised} {Controlled} {Trials}? {A} {Systematic} {Review}},
	volume = {185},
	issn = {0025-729X},
	shorttitle = {Does the {CONSORT} checklist improve the quality of reports of randomised controlled trials?},
	doi = {10.5694/j.1326-5377.2006.tb00557.x},
	abstract = {OBJECTIVE: To determine whether the adoption of the CONSORT checklist is associated with improvement in the quality of reporting of randomised controlled trials (RCTs).
DATA SOURCES: MEDLINE, EMBASE, Cochrane CENTRAL, and reference lists of included studies and of experts were searched to identify eligible studies published between 1996 and 2005.
STUDY SELECTION: Studies were eligible if they (a) compared CONSORT-adopting and non-adopting journals after the publication of CONSORT, (b) compared CONSORT adopters before and after publication of CONSORT, or (c) a combination of (a) and (b). Outcomes examined included reports for any of the 22 items on the CONSORT checklist or overall trial quality.
DATA SYNTHESIS: 1128 studies were retrieved, of which 248 were considered possibly relevant. Eight studies were included in the review. CONSORT adopters had significantly better reporting of the method of sequence generation (risk ratio [RR], 1.67; 95\% CI, 1.19-2.33), allocation concealment (RR, 1.66; 95\% CI, 1.37-2.00) and overall number of CONSORT items than non-adopters (standardised mean difference, 0.83; 95\% CI, 0.46-1.19). CONSORT adoption had less effect on reporting of participant flow (RR, 1.14; 95\% CI, 0.89-1.46) and blinding of participants (RR, 1.09; 95\% CI, 0.84-1.43) or data analysts (RR, 5.44; 95\% CI, 0.73-36.87). In studies examining CONSORT-adopting journals before and after the publication of CONSORT, description of the method of sequence generation (RR, 2.78; 95\% CI, 1.78-4.33), participant flow (RR, 8.06; 95\% CI, 4.10-15.83), and total CONSORT items (standardised mean difference, 3.67 items; 95\% CI, 2.09-5.25) were improved after adoption of CONSORT by the journal.
CONCLUSIONS: Journal adoption of CONSORT is associated with improved reporting of RCTs.},
	language = {eng},
	number = {5},
	journal = {The Medical Journal of Australia},
	author = {Plint, Amy C. and Moher, David and Morrison, Andra and Schulz, Kenneth and Altman, Douglas G. and Hill, Catherine and Gaboury, Isabelle},
	month = sep,
	year = {2006},
	pmid = {16948622},
	keywords = {Editorial Policies, Humans, Periodicals as Topic, Publishing, Quality Control, Randomized Controlled Trials as Topic},
	pages = {263--267},
}

@article{gundersen_machine_2022,
	title = {Do {Machine} {Learning} {Platforms} {Provide} {Out}-of-the-{Box} {Reproducibility}?},
	volume = {126},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002090},
	doi = {10.1016/j.future.2021.06.014},
	abstract = {Science is experiencing an ongoing reproducibility crisis. In light of this crisis, our objective is to investigate whether machine learning platforms provide out-of-the-box reproducibility. Our method is twofold: First, we survey machine learning platforms for whether they provide features that simplify making experiments reproducible out-of-the-box. Second, we conduct the exact same experiment on four different machine learning platforms, and by this varying the processing unit and ancillary software only. The survey shows that no machine learning platform supports the feature set described by the proposed framework while the experiment reveals statstically significant difference in results when the exact same experiment is conducted on different machine learning platforms. The surveyed machine learning platforms do not on their own enable users to achieve the full reproducibility potential of their research. Also, the machine learning platforms with most users provide less functionality for achieving it. Furthermore, results differ when executing the same experiment on the different platforms. Wrong conclusions can be inferred at the at 95\% confidence level. Hence, we conclude that machine learning platforms do not provide reproducibility out-of-the-box and that results generated from one machine learning platform alone cannot be fully trusted.},
	language = {en},
	urldate = {2023-02-27},
	journal = {Future Generation Computer Systems},
	author = {Gundersen, Odd Erik and Shamsaliei, Saeid and Isdahl, Richard Juul},
	month = jan,
	year = {2022},
	keywords = {Machine learning, Reproducibility, Reproducibility experiment, Reproducible AI, Survey},
	pages = {34--47},
}

@article{roberts_common_2021,
	title = {Common {Pitfalls} and {Recommendations} for {Using} {Machine} {Learning} {To} {Detect} and {Prognosticate} for {COVID}-19 {Using} {Chest} {Radiographs} and {CT} {Scans}},
	volume = {3},
	copyright = {2021 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-021-00307-0},
	doi = {10.1038/s42256-021-00307-0},
	abstract = {Machine learning methods offer great promise for fast and accurate detection and prognostication of coronavirus disease 2019 (COVID-19) from standard-of-care chest radiographs (CXR) and chest computed tomography (CT) images. Many articles have been published in 2020 describing new machine learning-based models for both of these tasks, but it is unclear which are of potential clinical utility. In this systematic review, we consider all published papers and preprints, for the period from 1 January 2020 to 3 October 2020, which describe new machine learning models for the diagnosis or prognosis of COVID-19 from CXR or CT images. All manuscripts uploaded to bioRxiv, medRxiv and arXiv along with all entries in EMBASE and MEDLINE in this timeframe are considered. Our search identified 2,212 studies, of which 415 were included after initial screening and, after quality screening, 62 studies were included in this systematic review. Our review finds that none of the models identified are of potential clinical use due to methodological flaws and/or underlying biases. This is a major weakness, given the urgency with which validated COVID-19 models are needed. To address this, we give many recommendations which, if followed, will solve these issues and lead to higher-quality model development and well-documented manuscripts.},
	language = {en},
	number = {3},
	urldate = {2021-06-23},
	journal = {Nature Machine Intelligence},
	author = {Roberts, Michael and Driggs, Derek and Thorpe, Matthew and Gilbey, Julian and Yeung, Michael and Ursprung, Stephan and Aviles-Rivero, Angelica I. and Etmann, Christian and McCague, Cathal and Beer, Lucian and Weir-McCall, Jonathan R. and Teng, Zhongzhao and Gkrania-Klotsas, Effrossyni and Rudd, James H. F. and Sala, Evis and Schönlieb, Carola-Bibiane},
	month = mar,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 3
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Computational science;Diagnostic markers;Prognostic markers;SARS-CoV-2
Subject\_term\_id: computational-science;diagnostic-markers;prognostic-markers;sars-cov-2},
	pages = {199--217},
	file = {Snapshot:C\:\\Users\\pthan\\Zotero\\storage\\7XDH58JW\\s42256-021-00307-0.html:text/html},
}

@article{valletta_applications_2017,
	title = {Applications of {Machine} {Learning} in {Animal} {Behaviour} {Studies}},
	volume = {124},
	issn = {0003-3472},
	url = {https://www.sciencedirect.com/science/article/pii/S0003347216303360},
	doi = {10.1016/j.anbehav.2016.12.005},
	abstract = {In many areas of animal behaviour research, improvements in our ability to collect large and detailed data sets are outstripping our ability to analyse them. These diverse, complex and often high-dimensional data sets exhibit nonlinear dependencies and unknown interactions across multiple variables, and may fail to conform to the assumptions of many classical statistical methods. The field of machine learning provides methodologies that are ideally suited to the task of extracting knowledge from these data. In this review, we aim to introduce animal behaviourists unfamiliar with machine learning (ML) to the promise of these techniques for the analysis of complex behavioural data. We start by describing the rationale behind ML and review a number of animal behaviour studies where ML has been successfully deployed. The ML framework is then introduced by presenting several unsupervised and supervised learning methods. Following this overview, we illustrate key ML approaches by developing data analytical pipelines for three different case studies that exemplify the types of behavioural and ecological questions ML can address. The first uses a large number of spectral and morphological characteristics that describe the appearance of pheasant, Phasianus colchicus, eggs to assign them to putative clutches. The second takes a continuous data stream of feeder visits from PIT (passive integrated transponder)-tagged jackdaws, Corvus monedula, and extracts foraging events from it, which permits the construction of social networks. Our final example uses aerial images to train a classifier that detects the presence of wildebeest, Connochaetes taurinus, to count individuals in a population. With the advent of cheaper sensing and tracking technologies an unprecedented amount of data on animal behaviour is becoming available. We believe that ML will play a central role in translating these data into scientific knowledge and become a useful addition to the animal behaviourist's analytical toolkit.},
	language = {en},
	urldate = {2021-05-13},
	journal = {Animal Behaviour},
	author = {Valletta, John Joseph and Torney, Colin and Kings, Michael and Thornton, Alex and Madden, Joah},
	month = feb,
	year = {2017},
	keywords = {classification, machine learning, predictive modelling, animal behaviour data, clustering, dimensionality reduction, random forests, social networks, supervised learning, unsupervised learning},
	pages = {203--220},
	file = {ScienceDirect Snapshot:C\:\\Users\\pthan\\Zotero\\storage\\TVR42DV6\\S0003347216303360.html:text/html},
}

@article{han_checklist_2017,
	title = {A {Checklist} is {Associated} with {Increased} {Quality} of {Reporting} {Preclinical} {Biomedical} {Research}: {A} {Systematic} {Review}},
	volume = {12},
	issn = {1932-6203},
	shorttitle = {A checklist is associated with increased quality of reporting preclinical biomedical research},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5597130/},
	doi = {10.1371/journal.pone.0183591},
	abstract = {Irreproducibility of preclinical biomedical research has gained recent attention. It is suggested that requiring authors to complete a checklist at the time of manuscript submission would improve the quality and transparency of scientific reporting, and ultimately enhance reproducibility. Whether a checklist enhances quality and transparency in reporting preclinical animal studies, however, has not been empirically studied. Here we searched two highly cited life science journals, one that requires a checklist at submission (Nature) and one that does not (Cell), to identify in vivo animal studies. After screening 943 articles, a total of 80 articles were identified in 2013 (pre-checklist) and 2015 (post-checklist), and included for the detailed evaluation of reporting methodological and analytical information. We compared the quality of reporting preclinical animal studies between the two journals, accounting for differences between journals and changes over time in reporting. We find that reporting of randomization, blinding, and sample-size estimation significantly improved when comparing Nature to Cell from 2013 to 2015, likely due to implementation of a checklist. Specifically, improvement in reporting of the three methodological information was at least three times greater when a mandatory checklist was implemented than when it was not. Reporting the sex of animals and the number of independent experiments performed also improved from 2013 to 2015, likely from factors not related to a checklist. Our study demonstrates that completing a checklist at manuscript submission is associated with improved reporting of key methodological information in preclinical animal studies.},
	number = {9},
	urldate = {2022-07-09},
	journal = {PLoS ONE},
	author = {Han, SeungHye and Olonisakin, Tolani F. and Pribis, John P. and Zupetic, Jill and Yoon, Joo Heung and Holleran, Kyle M. and Jeong, Kwonho and Shaikh, Nader and Rubio, Doris M. and Lee, Janet S.},
	month = sep,
	year = {2017},
	pmid = {28902887},
	pmcid = {PMC5597130},
	pages = {e0183591},
}

@article{gundersen_reproducible_2018,
	title = {On {Reproducible} {AI}: {Towards} {Reproducible} {Research}, {Open} {Science}, and {Digital} {Scholarship} in {AI} {Publications}},
	volume = {39},
	copyright = {Copyright (c)  AI Magazine},
	issn = {2371-9621},
	shorttitle = {On {Reproducible} {AI}},
	url = {https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/2816},
	doi = {10.1609/aimag.v39i3.2816},
	abstract = {Background: Science is experiencing a reproducibility crisis. Artificial intelligence research is not an exception. Objective: To give practical and pragmatic recommendations for how to document AI research so that the results are reproducible. Method: Our analysis of the literature shows that AI publications fall short of providing enough documentation to facilitate reproducibility. Our suggested best practices are based on a framework for reproducibility and recommendations given for other disciplines. Results: We have made an author checklist based on our investigation and provided examples for how every item in the checklist can be documented. Conclusion: We encourage reviewers to use the suggested best practices and author checklist when reviewing submissions for AAAI publications and future AAAI conferences.},
	language = {en},
	number = {3},
	urldate = {2023-02-27},
	journal = {AI Magazine},
	author = {Gundersen, Odd Erik and Gil, Yolanda and Aha, David W.},
	month = sep,
	year = {2018},
	pages = {56--68},
}

@article{donoho_50_2017,
	title = {50 {Years} of {Data} {Science}},
	volume = {26},
	issn = {1061-8600},
	url = {https://doi.org/10.1080/10618600.2017.1384734},
	doi = {10.1080/10618600.2017.1384734},
	abstract = {More than 50 years ago, John Tukey called for a reformation of academic statistics. In “The Future of Data Analysis,” he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or “data analysis.” Ten to 20 years ago, John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland and Wu even suggested the catchy name “data science” for this envisioned field. A recent and growing phenomenon has been the emergence of “data science” programs at major universities, including UC Berkeley, NYU, MIT, and most prominently, the University of Michigan, which in September 2015 announced a \$100M “Data Science Initiative” that aims to hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; yet many academic statisticians perceive the new programs as “cultural appropriation.” This article reviews some ingredients of the current “data science moment,” including recent commentary about data science in the popular media, and about how/whether data science is really different from statistics. The now-contemplated field of data science amounts to a superset of the fields of statistics and machine learning, which adds some technology for “scaling up” to “big data.” This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next 50 years. Because all of science itself will soon become data that can be mined, the imminent revolution in data science is not about mere “scaling up,” but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts field-by-field. Drawing on work by Tukey, Cleveland, Chambers, and Breiman, I present a vision of data science based on the activities of people who are “learning from data,” and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today’s data science initiatives, while being able to accommodate the same short-term goals. Based on a presentation at the Tukey Centennial Workshop, Princeton, NJ, September 18, 2015.},
	number = {4},
	urldate = {2023-02-27},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Donoho, David},
	month = oct,
	year = {2017},
	keywords = {Data analysis, Correction, Cross-study analysis, Data science, Meta analysis, Predictive modeling, Quantitative programming environments, Statistics},
	pages = {745--766},
}

@article{mullainathan_machine_2017,
	title = {Machine {Learning}: {An} {Applied} {Econometric} {Approach}},
	volume = {31},
	issn = {0895-3309},
	shorttitle = {Machine {Learning}},
	url = {https://www.aeaweb.org/articles?id=10.1257/jep.31.2.87},
	doi = {10.1257/jep.31.2.87},
	abstract = {. This similarity to econometrics raises questions: How do these new empirical tools fit with what we know? As empirical economists, how can we use them? We present a way of thinking about machine learning that gives it its own place in the econometric toolbox. Machine learning not only provides new tools, it solves a different problem. Specifically, machine learning revolves around the problem of prediction, while many economic applications revolve around parameter estimation. So applying machine learning to economics requires finding relevant tasks. Machine learning algorithms are now technically easy to use: you can download convenient packages in R or Python. This also raises the risk that the algorithms are applied naively or their output is misinterpreted. We hope to make them conceptually easier to use by providing a crisper understanding of how these algorithms work, where they excel, and where they can stumble—and thus where they can be most usefully applied.},
	language = {en},
	number = {2},
	urldate = {2023-02-27},
	journal = {Journal of Economic Perspectives},
	author = {Mullainathan, Sendhil and Spiess, Jann},
	month = may,
	year = {2017},
	keywords = {Econometric Modeling: General, Econometric Software},
	pages = {87--106},
}

@inproceedings{rogers_just_2021,
	address = {Punta Cana, Dominican Republic},
	title = {`{Just} {What} do {You} {Think} {You}'re {Doing}, {Dave}?' {A} {Checklist} for {Responsible} {Data} {Use} in {NLP}},
	shorttitle = {`{Just} {What} do {You} {Think} {You}'re {Doing}, {Dave}?},
	url = {https://aclanthology.org/2021.findings-emnlp.414},
	doi = {10.18653/v1/2021.findings-emnlp.414},
	abstract = {A key part of the NLP ethics movement is responsible use of data, but exactly what that means or how it can be best achieved remain unclear. This position paper discusses the core legal and ethical principles for collection and sharing of textual data, and the tensions between them. We propose a potential checklist for responsible data (re-)use that could both standardise the peer review of conference submissions, as well as enable a more in-depth view of published research across the community. Our proposal aims to contribute to the development of a consistent standard for data (re-)use, embraced across NLP conferences.},
	urldate = {2023-02-27},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Rogers, Anna and Baldwin, Timothy and Leins, Kobi},
	month = nov,
	year = {2021},
	pages = {4821--4833},
}

@article{liao_are_2021-1,
	title = {Are {We} {Learning} {Yet}? {A} {Meta} {Review} of {Evaluation} {Failures} {Across} {Machine} {Learning}},
	volume = {1},
	shorttitle = {Are {We} {Learning} {Yet}?},
	url = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/757b505cfd34c64c85ca5b5690ee5293-Abstract-round2.html},
	language = {en},
	urldate = {2022-11-07},
	journal = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
	author = {Liao, Thomas and Taori, Rohan and Raji, Deborah and Schmidt, Ludwig},
	month = dec,
	year = {2021},
}

@inproceedings{raji_fallacy_2022-1,
	address = {Seoul Republic of Korea},
	title = {The {Fallacy} of {AI} {Functionality}},
	isbn = {978-1-4503-9352-2},
	url = {https://dl.acm.org/doi/10.1145/3531146.3533158},
	doi = {10.1145/3531146.3533158},
	language = {en},
	urldate = {2022-11-07},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Raji, Inioluwa Deborah and Kumar, I. Elizabeth and Horowitz, Aaron and Selbst, Andrew},
	month = jun,
	year = {2022},
	pages = {959--972},
}

@inproceedings{raff_step_2019,
	title = {A {Step} {Toward} {Quantifying} {Independently} {Reproducible} {Machine} {Learning} {Research}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/c429429bf1f2af051f2021dc92a8ebea-Abstract.html},
	abstract = {What makes a paper independently reproducible? Debates on reproducibility center around intuition or assumptions but lack empirical results. Our field focuses on releasing code, which is important, but is not sufficient for determining reproducibility. We take the first step toward a quantifiable answer by manually attempting to implement 255 papers published from 1984 until 2017, recording features of each paper, and performing statistical analysis of the results. For each paper, we did not look at the authors code, if released, in order to prevent bias toward discrepancies between code and paper.},
	urldate = {2022-11-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Raff, Edward},
	year = {2019},
}

@article{white_strengthening_2015,
	title = {Strengthening the {Reporting} of {Observational} {Studies} in {Epidemiology} for respondent-driven sampling studies: “{STROBE}-{RDS}” statement},
	volume = {68},
	issn = {0895-4356},
	shorttitle = {Strengthening the {Reporting} of {Observational} {Studies} in {Epidemiology} for respondent-driven sampling studies},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435615001717},
	doi = {10.1016/j.jclinepi.2015.04.002},
	abstract = {Objectives
Respondent-driven sampling (RDS) is a new data collection methodology used to estimate characteristics of hard-to-reach groups, such as the HIV prevalence in drug users. Many national public health systems and international organizations rely on RDS data. However, RDS reporting quality and available reporting guidelines are inadequate. We carried out a systematic review of RDS studies and present Strengthening the Reporting of Observational Studies in Epidemiology for RDS Studies (STROBE-RDS), a checklist of essential items to present in RDS publications, justified by an explanation and elaboration document.
Study Design and Setting
We searched the MEDLINE (1970–2013), EMBASE (1974–2013), and Global Health (1910–2013) databases to assess the number and geographical distribution of published RDS studies. STROBE-RDS was developed based on STROBE guidelines, following Guidance for Developers of Health Research Reporting Guidelines.
Results
RDS has been used in over 460 studies from 69 countries, including the USA (151 studies), China (70), and India (32). STROBE-RDS includes modifications to 12 of the 22 items on the STROBE checklist. The two key areas that required modification concerned the selection of participants and statistical analysis of the sample.
Conclusion
STROBE-RDS seeks to enhance the transparency and utility of research using RDS. If widely adopted, STROBE-RDS should improve global infectious diseases public health decision making.},
	language = {en},
	number = {12},
	urldate = {2022-11-02},
	journal = {Journal of Clinical Epidemiology},
	author = {White, Richard G. and Hakim, Avi J. and Salganik, Matthew J. and Spiller, Michael W. and Johnston, Lisa G. and Kerr, Ligia and Kendall, Carl and Drake, Amy and Wilson, David and Orroth, Kate and Egger, Matthias and Hladik, Wolfgang},
	month = dec,
	year = {2015},
	keywords = {Humans, Biomedical research/methods, Cross-sectional studies, Epidemiologic research design, Epidemiologic studies, Guidelines as topic, Guidelines as topic/standards, Observation/methods, Practice guidelines as topic, Publishing/standards, Research design},
	pages = {1463--1471},
}

@article{mongan_checklist_2020,
	title = {Checklist for {Artificial} {Intelligence} in {Medical} {Imaging} ({CLAIM}): {A}                     {Guide} for {Authors} and {Reviewers}},
	volume = {2},
	shorttitle = {Checklist for {Artificial} {Intelligence} in {Medical} {Imaging} ({CLAIM})},
	url = {https://pubs.rsna.org/doi/10.1148/ryai.2020200029},
	doi = {10.1148/ryai.2020200029},
	number = {2},
	urldate = {2022-01-17},
	journal = {Radiology: Artificial Intelligence},
	author = {Mongan, John and Moy, Linda and Kahn, Charles                             E.},
	month = mar,
	year = {2020},
	note = {Publisher: Radiological Society of North America},
	pages = {e200029},
}

@misc{noauthor_principles_2015,
	title = {Principles and {Guidelines} for {Reporting} {Preclinical} {Research}},
	url = {https://www.nih.gov/research-training/rigor-reproducibility/principles-guidelines-reporting-preclinical-research},
	abstract = {Rigorous statistical analysis, transparency in reporting, data and material sharing, consideration of refutations, and consider establishing best practice guidelines.},
	language = {EN},
	urldate = {2022-10-27},
	journal = {National Institutes of Health (NIH)},
	month = aug,
	year = {2015},
}

@misc{kapoor_leakage_2022,
	title = {Leakage and the {Reproducibility} {Crisis} in {ML}-based {Science}},
	url = {http://arxiv.org/abs/2207.07048},
	doi = {10.48550/arXiv.2207.07048},
	abstract = {The use of machine learning (ML) methods for prediction and forecasting has become widespread across the quantitative sciences. However, there are many known methodological pitfalls, including data leakage, in ML-based science. In this paper, we systematically investigate reproducibility issues in ML-based science. We show that data leakage is indeed a widespread problem and has led to severe reproducibility failures. Specifically, through a survey of literature in research communities that adopted ML methods, we find 17 fields where errors have been found, collectively affecting 329 papers and in some cases leading to wildly overoptimistic conclusions. Based on our survey, we present a fine-grained taxonomy of 8 types of leakage that range from textbook errors to open research problems. We argue for fundamental methodological changes to ML-based science so that cases of leakage can be caught before publication. To that end, we propose model info sheets for reporting scientific claims based on ML models that would address all types of leakage identified in our survey. To investigate the impact of reproducibility errors and the efficacy of model info sheets, we undertake a reproducibility study in a field where complex ML models are believed to vastly outperform older statistical models such as Logistic Regression (LR): civil war prediction. We find that all papers claiming the superior performance of complex ML models compared to LR models fail to reproduce due to data leakage, and complex ML models don't perform substantively better than decades-old LR models. While none of these errors could have been caught by reading the papers, model info sheets would enable the detection of leakage in each case.},
	urldate = {2022-10-27},
	publisher = {arXiv},
	author = {Kapoor, Sayash and Narayanan, Arvind},
	month = jul,
	year = {2022},
	note = {arXiv:2207.07048 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Methodology},
}

@misc{bell_perspectives_2021,
	title = {Perspectives on {Machine} {Learning} from {Psychology}'s {Reproducibility} {Crisis}},
	url = {http://arxiv.org/abs/2104.08878},
	doi = {10.48550/arXiv.2104.08878},
	abstract = {In the early 2010s, a crisis of reproducibility rocked the field of psychology. Following a period of reflection, the field has responded with radical reform of its scientific practices. More recently, similar questions about the reproducibility of machine learning research have also come to the fore. In this short paper, we present select ideas from psychology's reformation, translating them into relevance for a machine learning audience.},
	urldate = {2022-10-27},
	publisher = {arXiv},
	author = {Bell, Samuel J. and Kampman, Onno P.},
	month = apr,
	year = {2021},
	note = {arXiv:2104.08878 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@article{johnson_reproducibility_2017,
	title = {On the {Reproducibility} of {Psychological} {Science}},
	volume = {112},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2016.1240079},
	doi = {10.1080/01621459.2016.1240079},
	abstract = {Investigators from a large consortium of scientists recently performed a multi-year study in which they replicated 100 psychology experiments. Although statistically significant results were reported in 97\% of the original studies, statistical significance was achieved in only 36\% of the replicated studies. This article presents a reanalysis of these data based on a formal statistical model that accounts for publication bias by treating outcomes from unpublished studies as missing data, while simultaneously estimating the distribution of effect sizes for those studies that tested nonnull effects. The resulting model suggests that more than 90\% of tests performed in eligible psychology experiments tested negligible effects, and that publication biases based on p-values caused the observed rates of nonreproducibility. The results of this reanalysis provide a compelling argument for both increasing the threshold required for declaring scientific discoveries and for adopting statistical summaries of evidence that account for the high proportion of tested hypotheses that are false. Supplementary materials for this article are available online.},
	number = {517},
	urldate = {2022-10-27},
	journal = {Journal of the American Statistical Association},
	author = {Johnson, Valen E. and Payne, Richard D. and Wang, Tianying and Asher, Alex and Mandal, Soutrik},
	month = jan,
	year = {2017},
	pmid = {29861517},
	keywords = {Reproducibility, Bayes factor, Null hypothesis significance test, Posterior model probability, Publication bias, Significance test},
	pages = {1--10},
}

@article{banja_ai_2020,
	title = {{AI} {Hype} and {Radiology}: {A} {Plea} for {Realism} and {Accuracy}},
	volume = {2},
	issn = {2638-6100},
	shorttitle = {{AI} {Hype} and {Radiology}},
	url = {http://pubs.rsna.org/doi/10.1148/ryai.2020190223},
	doi = {10.1148/ryai.2020190223},
	language = {en},
	number = {4},
	urldate = {2022-10-27},
	journal = {Radiology: Artificial Intelligence},
	author = {Banja, John},
	month = jul,
	year = {2020},
	pages = {e190223},
}

@article{liu_successes_2019-1,
	title = {Successes and {Struggles} with {Computational} {Reproducibility}: {Lessons} from the {Fragile} {Families} {Challenge}},
	volume = {5},
	issn = {2378-0231, 2378-0231},
	shorttitle = {Successes and {Struggles} with {Computational} {Reproducibility}},
	url = {http://journals.sagepub.com/doi/10.1177/2378023119849803},
	doi = {10.1177/2378023119849803},
	abstract = {Reproducibility is fundamental to science, and an important component of reproducibility is computational reproducibility: the ability of a researcher to recreate the results of a published study using the original author’s raw data and code. Although most people agree that computational reproducibility is important, it is still difficult to achieve in practice. In this article, the authors describe their approach to enabling computational reproducibility for the 12 articles in this special issue of Socius about the Fragile Families Challenge. The approach draws on two tools commonly used by professional software engineers but not widely used by academic researchers: software containers (e.g., Docker) and cloud computing (e.g., Amazon Web Services). These tools made it possible to standardize the computing environment around each submission, which will ease computational reproducibility both today and in the future. Drawing on their successes and struggles, the authors conclude with recommendations to researchers and journals.},
	language = {en},
	urldate = {2022-10-27},
	journal = {Socius: Sociological Research for a Dynamic World},
	author = {Liu, David M. and Salganik, Matthew J.},
	month = jan,
	year = {2019},
	pages = {237802311984980},
}
